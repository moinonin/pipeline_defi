{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas import DataFrame\n",
    "import pickle, random\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0 = pd.read_csv('../spreadsheets/rlhf_1064.csv') # 0.005, 0.75, 0.1, 0.95, 0.999, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_159nlp.csv') # second Best 0.01, 0.85, 0.01, 0.95, 0.95, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_07rl.csv') # Best\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_1072.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/shufled_rlhf_11rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_12rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_15rl.csv')\n",
    "df0 = pd.read_csv('../spreadsheets/rlhf_small_154nlp.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_19rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_24rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_23rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_25rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_26rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_157nlp.csv') # 0.7, 0.95, 0.5, 0.999, 0.99, 16000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_157nlpgate.csv') # 0.25, 0.95, 0.01, 0.997, 0.999, 14000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_29rl.csv') # 0.9, 0.9, 0.005, 0.95, 0.999, 10000,\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_30rl.csv') # 0.005, 0.75, 0.1, 0.95, 0.999, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_32rl.csv') # 0.01, 0.85, 0.01, 0.95, 0.95, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_33rl.csv')# 0.05, 0.85, 0.01, 0.997, 0.95, 4000\n",
    "# df0 = pd.read_csv('../spreadsheets/rlhf_small_36rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_39rl.csv')# 0.05, 0.85, 0.01, 0.997, 0.95, 4000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_154nlp_refined.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_27rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_43rl.csv') # 0.001, 0.99, 1.0, 0.95, 0.99, 10000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_1064_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['action'] = df0['action'].replace('go_long', 'do_nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sma-05 (entry)</th>\n",
       "      <th>sma-07 (entry)</th>\n",
       "      <th>sma-25 (entry)</th>\n",
       "      <th>sma-compare (entry)</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>imit-action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26.51860</td>\n",
       "      <td>26.494286</td>\n",
       "      <td>26.480640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>12.655161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.47140</td>\n",
       "      <td>5.535714</td>\n",
       "      <td>5.552160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>38.210539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.122729</td>\n",
       "      <td>0.125424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.774026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.29758</td>\n",
       "      <td>2.302243</td>\n",
       "      <td>2.299924</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>-2.632130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>43.28800</td>\n",
       "      <td>43.291429</td>\n",
       "      <td>42.727200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>38.482866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sma-05 (entry)  sma-07 (entry)  sma-25 (entry)  \\\n",
       "0           0        26.51860       26.494286       26.480640   \n",
       "1           1         5.47140        5.535714        5.552160   \n",
       "2           2         0.12230        0.122729        0.125424   \n",
       "3           3         2.29758        2.302243        2.299924   \n",
       "4           4        43.28800       43.291429       42.727200   \n",
       "\n",
       "   sma-compare (entry)  is_short    action imit-action   nlpreds     reward  \n",
       "0                    0         0   go_long    go_short   go_long  12.655161  \n",
       "1                    1         0   go_long    go_short   go_long  38.210539  \n",
       "2                    1         1  go_short    go_short   go_long  -5.774026  \n",
       "3                    0         1  go_short    go_short  go_short  -2.632130  \n",
       "4                    0         0   go_long    go_short  go_short  38.482866  "
      ]
     },
     "execution_count": 4329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "go_short    641\n",
       "go_long     450\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['refined-action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action\n",
       "go_short    651\n",
       "go_long     440\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df: DataFrame) -> DataFrame:\n",
    "    train_data = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        col_name = col.split(' ')[0]\n",
    "        train_data[f'{col_name}'] = df[col]\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf0 = pd.DataFrame()\n",
    "train_data = prep_data(df0) if newdf0.empty else prep_data(newdf0)\n",
    "#train_data = prep_data(newdf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>sma-25</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>predicted_action</th>\n",
       "      <th>reward</th>\n",
       "      <th>refined-action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.018487e+04</td>\n",
       "      <td>2.024511e+04</td>\n",
       "      <td>26.5186</td>\n",
       "      <td>26.494286</td>\n",
       "      <td>26.48064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>12.655161</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.410609e+06</td>\n",
       "      <td>1.440998e+06</td>\n",
       "      <td>5.4714</td>\n",
       "      <td>5.535714</td>\n",
       "      <td>5.55216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>38.210539</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ask           bid   sma-05     sma-07    sma-25  sma-compare  \\\n",
       "0  2.018487e+04  2.024511e+04  26.5186  26.494286  26.48064            0   \n",
       "1  1.410609e+06  1.440998e+06   5.4714   5.535714   5.55216            1   \n",
       "\n",
       "   is_short   action     nlpreds predicted_action     reward refined-action  \n",
       "0         0  go_long  do_nothing       do_nothing  12.655161        go_long  \n",
       "1         0  go_long    go_short         go_short  38.210539        go_long  "
      ]
     },
     "execution_count": 4335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode actions into numerical values\n",
    "action_mapping = {\"go_long\": 0, \"go_short\": 1, \"do_nothing\": 2}\n",
    "action_col = 'nlpreds' if newdf0.empty else 'refined-action'\n",
    "train_data[\"action_num\"] = train_data[f\"{action_col}\"].map(action_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data[\"action_num\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL parameters\n",
    "states = train_data[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values  # Include binary_state\n",
    "actions = list(action_mapping.values())  # Action space\n",
    "rewards = train_data[\"reward\"].values  # Rewards\n",
    "n_states = states.shape[0]\n",
    "n_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.95, 1.0, 0.99, 0.99, 10000]\n"
     ]
    }
   ],
   "source": [
    "list_1 = [\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "]\n",
    "\n",
    "list_2 = [\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "]\n",
    "\n",
    "# Combine the lists and remove duplicates\n",
    "combined_set = {tuple(sublist) for sublist in list_1 + list_2}\n",
    "\n",
    "# Convert the set back to a list of lists\n",
    "combined_list = [list(sublist) for sublist in combined_set]\n",
    "\n",
    "# Print the combined list\n",
    "for sublist in combined_list:\n",
    "    print(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4341,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameters = [\n",
    "    [0.001, 0.99, 1.0, 0.95, 0.99, 10000],\n",
    "    [1, 0.9, 1.0, 0.99, 0.99, 11000],\n",
    "    [0.1, 0.9, 0.1, 0.99, 0.995, 4000],\n",
    "    [0.005, 0.75, 0.1, 0.95, 0.999, 12000],\n",
    "    [0.001, 0.75, 1.0, 0.99, 0.99, 30000],\n",
    "    [1, 0.75, 0.005, 0.95, 0.95, 22000],\n",
    "    [0.01, 0.99, 1.0, 0.95, 0.99, 16000],\n",
    "    [0.7, 0.99, 1.0, 0.95, 0.997, 8000],\n",
    "    [0.01, 0.95, 1.0, 0.997, 0.995, 26000],\n",
    "    [0.25, 0.95, 0.01, 0.997, 0.999, 14000],\n",
    "    [0.5, 0.85, 0.5, 0.997, 0.997, 14000],\n",
    "    [0.01, 0.85, 0.01, 0.95, 0.95, 12000],\n",
    "    [0.9, 0.99, 0.5, 0.995, 0.95, 12000],\n",
    "    [0.05, 0.9, 0.5, 0.95, 0.999, 4000],\n",
    "    [0.05, 0.99, 0.5, 0.99, 0.997, 6000],\n",
    "    [1, 0.75, 0.05, 0.999, 0.999, 10000],\n",
    "    [0.9, 0.95, 1.0, 0.99, 0.99, 8000],\n",
    "    [0.25, 0.75, 0.01, 0.995, 0.999, 20000],\n",
    "    [0.3, 0.75, 1.0, 0.995, 0.99, 10000],\n",
    "    [1, 0.9, 1.0, 0.999, 0.999, 10000],\n",
    "    [0.7, 0.75, 1.0, 0.97, 0.999, 28000],\n",
    "    [0.05, 0.95, 1.0, 0.999, 0.995, 12000],\n",
    "    [0.7, 0.95, 0.5, 0.999, 0.99, 16000],\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "    [0.25, 0.99, 0.01, 0.997, 0.99, 8000],\n",
    "    [1, 0.95, 0.1, 0.96, 0.96, 12000],\n",
    "    [0.9, 0.9, 0.005, 0.95, 0.999, 10000],\n",
    "    [0.05, 0.85, 0.01, 0.997, 0.95, 4000],\n",
    "    [0.01, 0.9, 0.5, 0.999, 0.999, 1500],\n",
    "    [1, 0.85, 1.0, 0.95, 0.997, 21000],\n",
    "    [0.7, 0.9, 0.05, 0.95, 0.95, 20000]\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [0.001, 0.99, 1.0, 0.95, 0.99, 10000])\n",
      "(1, [1, 0.9, 1.0, 0.99, 0.99, 11000])\n",
      "(2, [0.1, 0.9, 0.1, 0.99, 0.995, 4000])\n",
      "(3, [0.005, 0.75, 0.1, 0.95, 0.999, 12000])\n",
      "(4, [0.001, 0.75, 1.0, 0.99, 0.99, 30000])\n",
      "(5, [1, 0.75, 0.005, 0.95, 0.95, 22000])\n",
      "(6, [0.01, 0.99, 1.0, 0.95, 0.99, 16000])\n",
      "(7, [0.7, 0.99, 1.0, 0.95, 0.997, 8000])\n",
      "(8, [0.01, 0.95, 1.0, 0.997, 0.995, 26000])\n",
      "(9, [0.25, 0.95, 0.01, 0.997, 0.999, 14000])\n",
      "(10, [0.5, 0.85, 0.5, 0.997, 0.997, 14000])\n",
      "(11, [0.01, 0.85, 0.01, 0.95, 0.95, 12000])\n",
      "(12, [0.9, 0.99, 0.5, 0.995, 0.95, 12000])\n",
      "(13, [0.05, 0.9, 0.5, 0.95, 0.999, 4000])\n",
      "(14, [0.05, 0.99, 0.5, 0.99, 0.997, 6000])\n",
      "(15, [1, 0.75, 0.05, 0.999, 0.999, 10000])\n",
      "(16, [0.9, 0.95, 1.0, 0.99, 0.99, 8000])\n",
      "(17, [0.25, 0.75, 0.01, 0.995, 0.999, 20000])\n",
      "(18, [0.3, 0.75, 1.0, 0.995, 0.99, 10000])\n",
      "(19, [1, 0.9, 1.0, 0.999, 0.999, 10000])\n",
      "(20, [0.7, 0.75, 1.0, 0.97, 0.999, 28000])\n",
      "(21, [0.05, 0.95, 1.0, 0.999, 0.995, 12000])\n",
      "(22, [0.7, 0.95, 0.5, 0.999, 0.99, 16000])\n",
      "(23, [0.25, 0.95, 1.0, 0.99, 0.99, 10000])\n",
      "(24, [0.25, 0.99, 0.01, 0.997, 0.99, 8000])\n",
      "(25, [1, 0.95, 0.1, 0.96, 0.96, 12000])\n",
      "(26, [0.9, 0.9, 0.005, 0.95, 0.999, 10000])\n",
      "(27, [0.05, 0.85, 0.01, 0.997, 0.95, 4000])\n",
      "(28, [0.01, 0.9, 0.5, 0.999, 0.999, 1500])\n",
      "(29, [1, 0.85, 1.0, 0.95, 0.997, 21000])\n",
      "(30, [0.7, 0.9, 0.05, 0.95, 0.95, 20000])\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(Hyperparameters):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3, 0.75, 1.0, 0.995, 0.99, 10000\n",
    "'''\n",
    "alpha = 0.7\n",
    "gamma = 0.75\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.97\n",
    "decay_rate = 0.97\n",
    "n_episodes = 28000\n",
    "n_states = states.shape[0]  # Number of states\n",
    "n_actions = len(actions)  # Number of actions\n",
    "'''\n",
    "alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes = Hyperparameters[18] # Hyperparameters[30] # Hyperparameters[22] #Hyperparameters[9] # Hyperparameters[21]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_index_mapping(df):\n",
    "    state_to_index = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        state = (row['sma-05'], row['sma-07'], row['sma-25'], row['sma-compare'], row['is_short'])\n",
    "        state_to_index[state] = idx\n",
    "    return state_to_index\n",
    "\n",
    "# Assuming 'df' is your dataframe used during training\n",
    "state_to_index = create_state_index_mapping(train_data)\n",
    "\n",
    "# Save the state_to_index dictionary for later use\n",
    "np.save('small_state_to_index.npy', state_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to choose an action using epsilon-greedy\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, n_actions)  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit: best known action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   0%|          | 28/10000 [00:00<00:36, 272.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000 - Total Reward: 340.60188608000016, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   9%|▊         | 859/10000 [00:03<00:33, 274.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800/10000 - Total Reward: 287.13005024, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  17%|█▋        | 1681/10000 [00:06<00:30, 270.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600/10000 - Total Reward: 1294.2525211700033, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  25%|██▍       | 2465/10000 [00:08<00:27, 277.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400/10000 - Total Reward: 287.13005024, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  33%|███▎      | 3272/10000 [00:11<00:25, 267.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200/10000 - Total Reward: 1375.9142074200038, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  41%|████      | 4086/10000 [00:14<00:21, 269.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000/10000 - Total Reward: 573.6837153199994, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  48%|████▊     | 4830/10000 [00:17<00:19, 267.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4800/10000 - Total Reward: 776.0243170100002, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  56%|█████▋    | 5632/10000 [00:20<00:15, 282.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5600/10000 - Total Reward: 301.08818041, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  64%|██████▍   | 6437/10000 [00:23<00:13, 260.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6400/10000 - Total Reward: 576.4260187499993, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  72%|███████▏  | 7233/10000 [00:26<00:09, 285.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7200/10000 - Total Reward: 492.8938515699997, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  81%|████████  | 8055/10000 [00:29<00:07, 264.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8000/10000 - Total Reward: 1225.761570230001, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  89%|████████▊ | 8863/10000 [00:32<00:04, 251.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8800/10000 - Total Reward: -101.13610701999839, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  97%|█████████▋| 9682/10000 [00:35<00:01, 287.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9600/10000 - Total Reward: 217.25053101, Epsilon: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...: 100%|██████████| 10000/10000 [00:36<00:00, 272.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simplified environment function\n",
    "def environment_step(current_state, action):\n",
    "    \"\"\"\n",
    "    Simulates the environment's response to an action.\n",
    "    \n",
    "    Args:\n",
    "        current_state (int): The current state of the environment.\n",
    "        action (int): The action taken by the agent.\n",
    "    \n",
    "    Returns:\n",
    "        next_state (int): The next state after taking the action.\n",
    "        reward (float): The reward received after taking the action.\n",
    "    \"\"\"\n",
    "    # Define the environment logic here\n",
    "    next_state = current_state + 1  # Example: Move to the next state\n",
    "    reward = rewards[next_state]    # Example: Reward is based on the next state\n",
    "    \n",
    "    return next_state, reward\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize a list to store rewards per episode\n",
    "rewards_per_episode = []\n",
    "\n",
    "# Training loop\n",
    "for episode in tqdm(range(n_episodes), desc=\"evaluating results per episode ...\"):\n",
    "    current_state = np.random.randint(0, n_states)  # Random initial state\n",
    "    total_reward = 0  # Initialize total reward for the current episode\n",
    "\n",
    "    while current_state < n_states - 1:\n",
    "        action = choose_action(current_state, epsilon)\n",
    "        \n",
    "        # Use the environment function to get the next state and reward\n",
    "        next_state, reward = environment_step(current_state, action)\n",
    "\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        q_table[current_state, action] += alpha * (\n",
    "            reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "        )\n",
    "        \n",
    "        total_reward += reward  # Accumulate reward for the current episode\n",
    "        current_state = next_state  # Move to next state\n",
    "\n",
    "    rewards_per_episode.append(total_reward)  # Store the total reward for the current episode\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if episode % 800 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Episode {episode}/{n_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "# Example: Save the Q-table\n",
    "np.save(\"small_q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_q_table(file_path):\n",
    "    return np.load(file_path)\n",
    "\n",
    "def load_state_index_mapping(file_path):\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "loaded_mapping = load_state_index_mapping(file_path=\"small_state_to_index.npy\")\n",
    "loaded_qtable = load_q_table(file_path=\"small_q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_state(sma_05: float, sma_07: float, sma_25: float, sma_compare: int, is_short: int):\n",
    "    state = np.array([[sma_05, sma_07, sma_25, sma_compare, is_short]])\n",
    "    if not np.all(np.isfinite(state)):\n",
    "        state = np.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_action(state, q_table, state_to_index, action_mapping, default_action: str = None):\n",
    "    state_tuple = tuple(state.flatten())\n",
    "\n",
    "    state_index = state_to_index.get(state_tuple, -1)\n",
    "\n",
    "    if not state_index == -1:\n",
    "        try:\n",
    "            q_values = q_table[state_index]\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            #return default_action\n",
    "    else:\n",
    "        state_tuples = list(state_to_index.keys())\n",
    "        kdtree = KDTree(state_tuples)\n",
    "        distance, index = kdtree.query(state.flatten())\n",
    "        nearest_state_tuple = state_tuples[index]\n",
    "        new_state_index = state_to_index[nearest_state_tuple]\n",
    "        q_values = loaded_qtable[new_state_index]\n",
    "    \n",
    "    #q_values = q_table[state_index]\n",
    "    best_action_index = np.argmax(q_values)\n",
    "    action = [action for action, index in action_mapping.items() if index == best_action_index][0]\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "with open(\"small_q_table.npy\", \"rb\") as f:\n",
    "    q_table = load_q_table(\"small_q_table.npy\")\n",
    "\n",
    "with open(\"small_state_to_index.npy\", \"rb\") as f:\n",
    "    state_to_index = load_state_index_mapping(\"small_state_to_index.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4351,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values\n",
    "state_1 = list(X[-1:].flatten()) # sample: [[0.87024    0.85277143 0.779504   0.         1.        ]]\n",
    "\n",
    "state = prep_state(*state_1)\n",
    "action_mapping = {\"go_long\": 0, \"go_short\": 1, \"do_nothing\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted action for the state is: go_long\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predicted_action = predict_action(state, q_table, state_to_index, action_mapping)\n",
    "    print(f\"The predicted action for the state is: {predicted_action}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"The state is not found in the state index mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict an entire range\n",
    "for idx, row in train_data.iterrows():\n",
    "    state = row[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values\n",
    "    action = predict_action(state, q_table, state_to_index, action_mapping)\n",
    "    train_data.loc[idx, \"predicted_action\"] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>sma-25</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>predicted_action</th>\n",
       "      <th>reward</th>\n",
       "      <th>refined-action</th>\n",
       "      <th>action_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.018487e+04</td>\n",
       "      <td>2.024511e+04</td>\n",
       "      <td>26.518600</td>\n",
       "      <td>26.494286</td>\n",
       "      <td>26.480640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>12.655161</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.410609e+06</td>\n",
       "      <td>1.440998e+06</td>\n",
       "      <td>5.471400</td>\n",
       "      <td>5.535714</td>\n",
       "      <td>5.552160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>38.210539</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.924476e+06</td>\n",
       "      <td>5.027070e+06</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.122729</td>\n",
       "      <td>0.125424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.774026</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.770843e+05</td>\n",
       "      <td>6.686044e+05</td>\n",
       "      <td>2.297580</td>\n",
       "      <td>2.302243</td>\n",
       "      <td>2.299924</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>-2.632130</td>\n",
       "      <td>go_short</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.693704e+03</td>\n",
       "      <td>2.677647e+03</td>\n",
       "      <td>43.288000</td>\n",
       "      <td>43.291429</td>\n",
       "      <td>42.727200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>38.482866</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.173899e+08</td>\n",
       "      <td>1.176989e+08</td>\n",
       "      <td>0.022337</td>\n",
       "      <td>0.022386</td>\n",
       "      <td>0.022462</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>-9.615395</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.153749e+05</td>\n",
       "      <td>2.155601e+05</td>\n",
       "      <td>9.232800</td>\n",
       "      <td>9.142000</td>\n",
       "      <td>8.893200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>19.469700</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.710601e+04</td>\n",
       "      <td>7.726149e+04</td>\n",
       "      <td>28.524400</td>\n",
       "      <td>28.671286</td>\n",
       "      <td>28.904200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>2.943566</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.247465e+04</td>\n",
       "      <td>9.264235e+04</td>\n",
       "      <td>1.383400</td>\n",
       "      <td>1.386429</td>\n",
       "      <td>1.377292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>-7.042068</td>\n",
       "      <td>go_short</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.880353e+03</td>\n",
       "      <td>7.872247e+03</td>\n",
       "      <td>24.172600</td>\n",
       "      <td>24.127571</td>\n",
       "      <td>23.724920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>45.194056</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.893794e+03</td>\n",
       "      <td>2.900306e+03</td>\n",
       "      <td>10.251800</td>\n",
       "      <td>10.255143</td>\n",
       "      <td>10.303840</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>26.485002</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.231441e+03</td>\n",
       "      <td>3.223359e+03</td>\n",
       "      <td>172.188000</td>\n",
       "      <td>172.408571</td>\n",
       "      <td>169.557200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>7.942358</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.121757e+04</td>\n",
       "      <td>1.125073e+04</td>\n",
       "      <td>75.875800</td>\n",
       "      <td>75.984714</td>\n",
       "      <td>76.118280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>-1.793259</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.844344e+04</td>\n",
       "      <td>4.840652e+04</td>\n",
       "      <td>14.420400</td>\n",
       "      <td>14.380429</td>\n",
       "      <td>14.279400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>15.220826</td>\n",
       "      <td>go_long</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.390529e+05</td>\n",
       "      <td>6.499554e+05</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.529429</td>\n",
       "      <td>1.491040</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>-4.656618</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ask           bid      sma-05      sma-07      sma-25  \\\n",
       "0   2.018487e+04  2.024511e+04   26.518600   26.494286   26.480640   \n",
       "1   1.410609e+06  1.440998e+06    5.471400    5.535714    5.552160   \n",
       "2   4.924476e+06  5.027070e+06    0.122300    0.122729    0.125424   \n",
       "3   6.770843e+05  6.686044e+05    2.297580    2.302243    2.299924   \n",
       "4   2.693704e+03  2.677647e+03   43.288000   43.291429   42.727200   \n",
       "5   1.173899e+08  1.176989e+08    0.022337    0.022386    0.022462   \n",
       "6   2.153749e+05  2.155601e+05    9.232800    9.142000    8.893200   \n",
       "7   7.710601e+04  7.726149e+04   28.524400   28.671286   28.904200   \n",
       "8   9.247465e+04  9.264235e+04    1.383400    1.386429    1.377292   \n",
       "9   7.880353e+03  7.872247e+03   24.172600   24.127571   23.724920   \n",
       "10  2.893794e+03  2.900306e+03   10.251800   10.255143   10.303840   \n",
       "11  3.231441e+03  3.223359e+03  172.188000  172.408571  169.557200   \n",
       "12  1.121757e+04  1.125073e+04   75.875800   75.984714   76.118280   \n",
       "13  4.844344e+04  4.840652e+04   14.420400   14.380429   14.279400   \n",
       "14  6.390529e+05  6.499554e+05    1.533000    1.529429    1.491040   \n",
       "\n",
       "    sma-compare  is_short    action     nlpreds predicted_action     reward  \\\n",
       "0             0         0   go_long  do_nothing       do_nothing  12.655161   \n",
       "1             1         0   go_long    go_short         go_short  38.210539   \n",
       "2             1         1  go_short     go_long          go_long  -5.774026   \n",
       "3             0         1  go_short    go_short         go_short  -2.632130   \n",
       "4             0         0   go_long  do_nothing       do_nothing  38.482866   \n",
       "5             1         1  go_short  do_nothing       do_nothing  -9.615395   \n",
       "6             0         0   go_long    go_short         go_short  19.469700   \n",
       "7             1         0   go_long    go_short         go_short   2.943566   \n",
       "8             0         1  go_short    go_short         go_short  -7.042068   \n",
       "9             0         0   go_long    go_short         go_short  45.194056   \n",
       "10            1         0   go_long    go_short         go_short  26.485002   \n",
       "11            0         0   go_long     go_long          go_long   7.942358   \n",
       "12            1         0   go_long  do_nothing       do_nothing  -1.793259   \n",
       "13            0         0   go_long    go_short         go_short  15.220826   \n",
       "14            0         1  go_short  do_nothing       do_nothing  -4.656618   \n",
       "\n",
       "   refined-action  action_num  \n",
       "0         go_long           0  \n",
       "1         go_long           0  \n",
       "2         go_long           0  \n",
       "3        go_short           1  \n",
       "4         go_long           0  \n",
       "5      do_nothing           2  \n",
       "6         go_long           0  \n",
       "7         go_long           0  \n",
       "8        go_short           1  \n",
       "9         go_long           0  \n",
       "10        go_long           0  \n",
       "11        go_long           0  \n",
       "12     do_nothing           2  \n",
       "13        go_long           0  \n",
       "14     do_nothing           2  "
      ]
     },
     "execution_count": 4354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_action\n",
       "go_long       998\n",
       "do_nothing     48\n",
       "go_short       45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['predicted_action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4356,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_data[(train_data['nlpreds'] == 'go_long') & (train_data['reward'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_short\n",
       "1    274\n",
       "0    186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['is_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "go_short    287\n",
       "go_long     194\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df0[(df0['reward'] > 0)]\n",
    "s['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed state 0/1090\n",
      "Current Predicted Reward: 0\n",
      "Current Actual Reward: 38.21053879\n",
      "Processed state 100/1090\n",
      "Current Predicted Reward: -1497.3221488599997\n",
      "Current Actual Reward: -1439.5238085599997\n",
      "Processed state 200/1090\n",
      "Current Predicted Reward: -951.1882966200004\n",
      "Current Actual Reward: -828.56395646\n",
      "Processed state 300/1090\n",
      "Current Predicted Reward: -688.2267154200002\n",
      "Current Actual Reward: -603.1857184999999\n",
      "Processed state 400/1090\n",
      "Current Predicted Reward: -596.0961439400002\n",
      "Current Actual Reward: -472.609580099999\n",
      "Processed state 500/1090\n",
      "Current Predicted Reward: -611.8604747499995\n",
      "Current Actual Reward: -552.2256943499984\n",
      "Processed state 600/1090\n",
      "Current Predicted Reward: -151.4634322099999\n",
      "Current Actual Reward: -184.6029190499986\n",
      "Processed state 700/1090\n",
      "Current Predicted Reward: -184.1245605299999\n",
      "Current Actual Reward: -267.82184308999854\n",
      "Processed state 800/1090\n",
      "Current Predicted Reward: -20.165833159999934\n",
      "Current Actual Reward: -271.7322811499985\n",
      "Processed state 900/1090\n",
      "Current Predicted Reward: -96.73355490999992\n",
      "Current Actual Reward: -332.4862911999984\n",
      "Processed state 1000/1090\n",
      "Current Predicted Reward: 75.42362627000009\n",
      "Current Actual Reward: -206.56167259999793\n",
      "Cumulative Predicted Reward: 347.83719075000016\n",
      "Cumulative Actual Reward: 18.10100048000213\n",
      "Prediction Efficiency: 1821.65%\n"
     ]
    }
   ],
   "source": [
    "# Performance measures\n",
    "# Initialize cumulative rewards\n",
    "cumulative_predicted_reward = 0\n",
    "cumulative_actual_reward = 0\n",
    "\n",
    "# Iterate through states to calculate rewards\n",
    "for state_index in range(n_states - 1):\n",
    "    # Predicted action from Q-table\n",
    "    predicted_action = np.argmax(q_table[state_index])  # Best action for the current state\n",
    "    # Actual action from the ground truth\n",
    "    actual_action = train_data[\"action_num\"].iloc[state_index]\n",
    "\n",
    "    # Get reward for predicted action only if it matches the actual action\n",
    "    if predicted_action == actual_action:\n",
    "        predicted_reward = rewards[state_index + 1]  # Reward for the correct prediction\n",
    "        cumulative_predicted_reward += predicted_reward\n",
    "\n",
    "    # Get actual reward for the ground truth action\n",
    "    actual_reward = rewards[state_index + 1]\n",
    "    cumulative_actual_reward += actual_reward\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if state_index % 100 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Processed state {state_index}/{n_states - 1}\")\n",
    "        print(f\"Current Predicted Reward: {cumulative_predicted_reward}\")\n",
    "        print(f\"Current Actual Reward: {cumulative_actual_reward}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cumulative Predicted Reward: {cumulative_predicted_reward}\")\n",
    "print(f\"Cumulative Actual Reward: {cumulative_actual_reward}\")\n",
    "\n",
    "# Optionally calculate efficiency\n",
    "efficiency = (\n",
    "    ((cumulative_predicted_reward - cumulative_actual_reward) / abs(cumulative_actual_reward)) * 100\n",
    "    if cumulative_actual_reward != 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "print(f\"Prediction Efficiency: {efficiency:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.42%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "correct_predictions = 0\n",
    "for state_index in range(n_states):\n",
    "    predicted_action = np.argmax(q_table[state_index])  # Predicted action\n",
    "    actual_action = train_data[\"action_num\"].iloc[state_index]  # Actual action\n",
    "    if predicted_action == actual_action:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / n_states\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[724   8   5]\n",
      " [274  37   3]\n",
      " [  0   0  40]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "y_true = train_data[\"action_num\"]  # Actual actions\n",
    "y_pred = [np.argmax(q_table[state_index]) for state_index in range(n_states)]  # Predicted actions\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_reward(action: str, is_short: int):\n",
    "    m = train_data[(train_data['predicted_action'] == f'{action}') & (train_data['is_short'] == is_short)]\n",
    "    counts = m['is_short'].value_counts()\n",
    "    total_reward = m['reward'].cumsum()[-1:].values[0]\n",
    "    wins = len(m[m['reward'] > 0])\n",
    "    losses = len(m[m['reward'] <= 0])\n",
    "    return {\n",
    "        'counts': counts.get(is_short),\n",
    "        'total reward': total_reward,\n",
    "        'winrate': f'{wins * 100 / (losses + wins):.2f}%',\n",
    "        'per trade profit': m[m['reward'] > 0]['reward'].sum() / wins,\n",
    "        'per trade loss': m[m['reward'] <= 0]['reward'].sum() / losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go_long 0: {'counts': 374, 'total reward': 1818.8337362700001, 'winrate': '49.73%', 'per trade profit': 18.950135804892472, 'per trade loss': -9.07389108212766}\n",
      "go_long 1: {'counts': 624, 'total reward': -815.3684768300008, 'winrate': '43.91%', 'per trade profit': 12.268098142810219, 'per trade loss': -11.933792479885714}\n",
      "go_short 0: {'counts': 33, 'total reward': -335.59366205999993, 'winrate': '24.24%', 'per trade profit': 24.723055178750002, 'per trade loss': -21.3351241396}\n",
      "go_short 1: {'counts': 12, 'total reward': -25.518345150000005, 'winrate': '41.67%', 'per trade profit': 11.535935124, 'per trade loss': -11.885431538571428}\n",
      "do_nothing 0: {'counts': 33, 'total reward': -463.45258031999987, 'winrate': '15.15%', 'per trade profit': 17.334968831999998, 'per trade loss': -19.64740801714286}\n",
      "do_nothing 1: {'counts': 15, 'total reward': -148.14451053999997, 'winrate': '20.00%', 'per trade profit': 13.592807729999999, 'per trade loss': -15.743577810833331}\n"
     ]
    }
   ],
   "source": [
    "dirs = [0,1]\n",
    "for action in action_mapping.keys():\n",
    "    for is_short in dirs:\n",
    "        try:\n",
    "            print(f'{action} {is_short}: {action_reward(action, is_short)}')\n",
    "        except IndexError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_action\n",
      "go_long       998\n",
      "do_nothing     48\n",
      "go_short       45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['predicted_action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corrected action\n",
    "def correct_action(row):\n",
    "    if row['predicted_action'] == 'go_long' and row['is_short'] == 1 and row['reward'] > 0:\n",
    "        return 'go_short'\n",
    "    if row['predicted_action'] == 'go_short' and row['is_short'] == 0 and row['reward'] > 0:\n",
    "        return 'go_long'\n",
    "    if row['predicted_action'] == 'do_nothing' and row['is_short'] == 1 and row['reward'] > 0:\n",
    "        return 'go_short'\n",
    "    if row['predicted_action'] == 'do_nothing' and row['is_short'] == 0 and row['reward'] > 0:\n",
    "        return 'go_long'\n",
    "    return row['predicted_action']\n",
    "\n",
    "def refiner_action(version: str, data: DataFrame = None) -> DataFrame:\n",
    "    data['refined-action'] = data.apply(lambda x: correct_action(x), axis=1)\n",
    "    # Validation: Ensure we're fixing the 751 misclassified entries\n",
    "    misclassified = data[\n",
    "        (data['predicted_action'] == 'go_long') & \n",
    "        (data['refined-action'] == 'go_short')\n",
    "    ]\n",
    "    # Add this after applying refined-action\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(\n",
    "        data['refined-action'], \n",
    "        data['predicted_action'],  # Assuming you have ground truth column\n",
    "        rownames=['refined'],\n",
    "        colnames=['predicted']\n",
    "    )\n",
    "\n",
    "    print(\"Updated Confusion Matrix:\")\n",
    "    print(confusion_matrix) \n",
    "    print(f\"Corrected {len(misclassified)} go_long->go_short misclassifications\")\n",
    "    \n",
    "    filename = f'../spreadsheets/rlhf_small_{version}_refined.csv'\n",
    "    data.to_csv(filename, index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4366,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/defi/Desktop/portfolio/projects/python/pipeline_defi/'\n",
    "#new_data = pd.read_csv('../spreadsheets/rlhf_small_154nlp.csv') \n",
    "def refine_file(version: str, file) -> DataFrame:\n",
    "    filename = f'{base_dir}{file}.csv'\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename)\n",
    "    #new_data = prep_data(df0.copy()) if newdf0.empty else prep_data(newdf0.copy())\n",
    "    new_data = prep_data(df)   \n",
    "    print(new_data.columns)\n",
    "    new_train_data = refiner_action(version=version, data=new_data)\n",
    "\n",
    "    #new_data = df0.copy()\n",
    "    print(new_train_data.columns)\n",
    "\n",
    "    new_train_data['nlpreds'] = new_train_data['predicted_action']\n",
    "    #new_data['predicted_action'] = new_train_data['predicted_action']\n",
    "    return new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/defi/Desktop/portfolio/projects/python/pipeline_defi/lean_df_154nlp.csv\n",
      "Index(['ask', 'bid', 'sma-05', 'sma-07', 'sma-25', 'sma-compare', 'is_short',\n",
      "       'action', 'nlpreds', 'predicted_action', 'reward'],\n",
      "      dtype='object')\n",
      "Updated Confusion Matrix:\n",
      "predicted   do_nothing  go_long  go_short\n",
      "refined                                  \n",
      "do_nothing          40        0         0\n",
      "go_long              5      724         8\n",
      "go_short             3      274        37\n",
      "Corrected 274 go_long->go_short misclassifications\n",
      "Index(['ask', 'bid', 'sma-05', 'sma-07', 'sma-25', 'sma-compare', 'is_short',\n",
      "       'action', 'nlpreds', 'predicted_action', 'reward', 'refined-action'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "newdf0 = refine_file('154nlp', 'lean_df_154nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_action\n",
      "go_long       998\n",
      "do_nothing     48\n",
      "go_short       45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(newdf0['predicted_action'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlpreds\n",
      "go_long       998\n",
      "do_nothing     48\n",
      "go_short       45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(newdf0['nlpreds'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>sma-25</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>predicted_action</th>\n",
       "      <th>reward</th>\n",
       "      <th>refined-action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.018487e+04</td>\n",
       "      <td>2.024511e+04</td>\n",
       "      <td>26.51860</td>\n",
       "      <td>26.494286</td>\n",
       "      <td>26.480640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>12.655161</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.410609e+06</td>\n",
       "      <td>1.440998e+06</td>\n",
       "      <td>5.47140</td>\n",
       "      <td>5.535714</td>\n",
       "      <td>5.552160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>38.210539</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.924476e+06</td>\n",
       "      <td>5.027070e+06</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.122729</td>\n",
       "      <td>0.125424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.774026</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.770843e+05</td>\n",
       "      <td>6.686044e+05</td>\n",
       "      <td>2.29758</td>\n",
       "      <td>2.302243</td>\n",
       "      <td>2.299924</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>-2.632130</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.693704e+03</td>\n",
       "      <td>2.677647e+03</td>\n",
       "      <td>43.28800</td>\n",
       "      <td>43.291429</td>\n",
       "      <td>42.727200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>38.482866</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ask           bid    sma-05     sma-07     sma-25  sma-compare  \\\n",
       "0  2.018487e+04  2.024511e+04  26.51860  26.494286  26.480640            0   \n",
       "1  1.410609e+06  1.440998e+06   5.47140   5.535714   5.552160            1   \n",
       "2  4.924476e+06  5.027070e+06   0.12230   0.122729   0.125424            1   \n",
       "3  6.770843e+05  6.686044e+05   2.29758   2.302243   2.299924            0   \n",
       "4  2.693704e+03  2.677647e+03  43.28800  43.291429  42.727200            0   \n",
       "\n",
       "   is_short    action     nlpreds predicted_action     reward refined-action  \n",
       "0         0   go_long  do_nothing       do_nothing  12.655161        go_long  \n",
       "1         0   go_long    go_short         go_short  38.210539        go_long  \n",
       "2         1  go_short     go_long          go_long  -5.774026        go_long  \n",
       "3         1  go_short    go_short         go_short  -2.632130       go_short  \n",
       "4         0   go_long  do_nothing       do_nothing  38.482866        go_long  "
      ]
     },
     "execution_count": 4370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.46411705\n"
     ]
    }
   ],
   "source": [
    "print(newdf0[(newdf0['nlpreds'] == 'go_short') & (newdf0['reward'] >  0)]['reward'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.46411705\n"
     ]
    }
   ],
   "source": [
    "print(train_data[(train_data['nlpreds'] == 'go_short') & (train_data['reward'] >  0)]['reward'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of rewards for each episode\n",
    "#rewards_per_episode = [...]  # Populate this with your actual data\n",
    "\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning with Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "def train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards):\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        current_state = np.random.randint(0, n_states)\n",
    "        total_reward = 0\n",
    "\n",
    "        while current_state < n_states - 1:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "\n",
    "            next_state = current_state + 1  # Adjust based on environment logic\n",
    "            reward = rewards[next_state]\n",
    "\n",
    "            best_next_action = np.argmax(q_table[next_state])\n",
    "            q_table[current_state, action] += alpha * (\n",
    "                reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    return q_table, rewards_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4375,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards):\n",
    "    correct_predictions = 0\n",
    "    total_reward = 0\n",
    "    reward_weighted_accuracy = []\n",
    "\n",
    "    for state_index in range(n_states):\n",
    "        predicted_action = np.argmax(q_table[state_index])  # Predicted action\n",
    "        actual_action = train_data[\"action_num\"].iloc[state_index]  # Actual action\n",
    "        reward = rewards[state_index]  # Reward for the action\n",
    "\n",
    "        if predicted_action == actual_action:\n",
    "            correct_predictions += 1\n",
    "            total_reward += reward\n",
    "\n",
    "        accuracy = correct_predictions / (state_index + 1)\n",
    "        reward_weighted_accuracy.append(total_reward / (state_index + 1))\n",
    "\n",
    "        # Optional: Log progress\n",
    "        if state_index % 100 == 0:\n",
    "            print(f\"Processed state {state_index}/{n_states} - Accuracy: {accuracy * 100:.2f}%, Reward-weighted Accuracy: {reward_weighted_accuracy[-1]}\")\n",
    "\n",
    "    final_reward_weighted_accuracy = total_reward / n_states\n",
    "    return final_reward_weighted_accuracy * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards):\n",
    "    # Initialize cumulative rewards\n",
    "    cumulative_predicted_reward = 0\n",
    "    cumulative_actual_reward = 0\n",
    "\n",
    "    # Iterate through states to calculate rewards\n",
    "    for state_index in range(n_states - 1):\n",
    "        # Predicted action from Q-table\n",
    "        predicted_action = np.argmax(q_table[state_index])  # Best action for the current state\n",
    "        # Actual action from the ground truth\n",
    "        actual_action = train_data[\"action_num\"].iloc[state_index]\n",
    "\n",
    "        # Get reward for predicted action only if it matches the actual action\n",
    "        if predicted_action == actual_action:\n",
    "            predicted_reward = rewards[state_index + 1]  # Reward for the correct prediction\n",
    "            cumulative_predicted_reward += predicted_reward\n",
    "\n",
    "        # Get actual reward for the ground truth action\n",
    "        actual_reward = rewards[state_index + 1]\n",
    "        cumulative_actual_reward += actual_reward\n",
    "    return cumulative_predicted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef random_search_reward_weighted(n_iter, param_grid, n_states, n_actions, rewards, train_data):\\n    best_params = None\\n    best_reward_weighted_accuracy = float(\\'-inf\\')\\n\\n    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\\n        alpha = random.choice(param_grid[\\'alpha\\'])\\n        gamma = random.choice(param_grid[\\'gamma\\'])\\n        epsilon = random.choice(param_grid[\\'epsilon\\'])\\n        min_epsilon = random.choice(param_grid[\\'decay_rate\\'])\\n        decay_rate = random.choice(param_grid[\\'decay_rate\\'])\\n        n_episodes = random.choice(param_grid[\\'n_episodes\\'])\\n\\n        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\\n        reward_weighted_accuracy = evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards)\\n\\n        if reward_weighted_accuracy > best_reward_weighted_accuracy:\\n            best_reward_weighted_accuracy = reward_weighted_accuracy\\n            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\\n\\n        print(f\"Iteration Reward-weighted Accuracy: {reward_weighted_accuracy:.2f}%, Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\\n\\n    return best_params, best_reward_weighted_accuracy\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'alpha\\': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\\n    \\'gamma\\': [0.75, 0.85, 0.9, 0.95, 0.99],\\n    \\'epsilon\\': [1.0, 0.5, 0.1, 0.05, 0.01],\\n    \\'min_epsilon\\': [0.05, 0.01, 0.005, 0.001, 0.001],\\n    \\'decay_rate\\': [0.95, 0.96, 0.97, 0.99, 0.995, 0.997, 0.999],\\n    \\'n_episodes\\': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 18000, 21000, 24000, 28000]\\n}\\n\\n# Perform Random Search\\nbest_params, best_reward_weighted_accuracy = random_search_reward_weighted(50, param_grid, n_states, n_actions, rewards, train_data)\\nprint(f\"Best Hyperparameters: {best_params}\")\\nprint(f\"Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\\n'"
      ]
     },
     "execution_count": 4377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def random_search_reward_weighted(n_iter, param_grid, n_states, n_actions, rewards, train_data):\n",
    "    best_params = None\n",
    "    best_reward_weighted_accuracy = float('-inf')\n",
    "\n",
    "    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\n",
    "        alpha = random.choice(param_grid['alpha'])\n",
    "        gamma = random.choice(param_grid['gamma'])\n",
    "        epsilon = random.choice(param_grid['epsilon'])\n",
    "        min_epsilon = random.choice(param_grid['decay_rate'])\n",
    "        decay_rate = random.choice(param_grid['decay_rate'])\n",
    "        n_episodes = random.choice(param_grid['n_episodes'])\n",
    "\n",
    "        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\n",
    "        reward_weighted_accuracy = evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards)\n",
    "\n",
    "        if reward_weighted_accuracy > best_reward_weighted_accuracy:\n",
    "            best_reward_weighted_accuracy = reward_weighted_accuracy\n",
    "            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\n",
    "\n",
    "        print(f\"Iteration Reward-weighted Accuracy: {reward_weighted_accuracy:.2f}%, Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\n",
    "\n",
    "    return best_params, best_reward_weighted_accuracy\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'gamma': [0.75, 0.85, 0.9, 0.95, 0.99],\n",
    "    'epsilon': [1.0, 0.5, 0.1, 0.05, 0.01],\n",
    "    'min_epsilon': [0.05, 0.01, 0.005, 0.001, 0.001],\n",
    "    'decay_rate': [0.95, 0.96, 0.97, 0.99, 0.995, 0.997, 0.999],\n",
    "    'n_episodes': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 18000, 21000, 24000, 28000]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "best_params, best_reward_weighted_accuracy = random_search_reward_weighted(50, param_grid, n_states, n_actions, rewards, train_data)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_data = newdf0\\ndef random_search_prediction_efficiency(n_iter, param_grid, n_states, n_actions, rewards, train_data):\\n    best_params = None\\n    best_cumulative_pred_reward = float(\\'-inf\\')\\n\\n    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\\n        alpha = random.choice(param_grid[\\'alpha\\'])\\n        gamma = random.choice(param_grid[\\'gamma\\'])\\n        epsilon = random.choice(param_grid[\\'epsilon\\'])\\n        min_epsilon = random.choice(param_grid[\\'decay_rate\\'])\\n        decay_rate = random.choice(param_grid[\\'decay_rate\\'])\\n        n_episodes = random.choice(param_grid[\\'n_episodes\\'])\\n\\n        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\\n        cumulative_pred_reward = evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards)\\n\\n        if cumulative_pred_reward > best_cumulative_pred_reward:\\n            best_cumulative_pred_reward = cumulative_pred_reward\\n            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\\n\\n        print(f\"Iteration cumulative predicted reward: {cumulative_pred_reward:.2f}%, Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\\n\\n    return best_params, best_cumulative_pred_reward\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'alpha\\': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\\n    \\'gamma\\': [0.75, 0.85, 0.9, 0.95, 0.99],\\n    \\'epsilon\\': [1.0, 0.5, 0.1, 0.05, 0.01, 0.005],\\n    \\'min_epsilon\\': [0.05, 0.01, 0.005, 0.001, 0.001],\\n    \\'decay_rate\\': [0.95, 0.99, 0.995, 0.997, 0.999],\\n    \\'n_episodes\\': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 16000, 18000, 20000, 22000, 24000, 26000, 28000, 30000]\\n}\\n\\n# Perform Random Search\\nbest_params, best_cumulative_pred_reward = random_search_prediction_efficiency(50, param_grid, n_states, n_actions, rewards, train_data)\\nprint(f\"Best Hyperparameters: {best_params}\")\\nprint(f\"Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\\n'"
      ]
     },
     "execution_count": 4378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_data = newdf0\n",
    "def random_search_prediction_efficiency(n_iter, param_grid, n_states, n_actions, rewards, train_data):\n",
    "    best_params = None\n",
    "    best_cumulative_pred_reward = float('-inf')\n",
    "\n",
    "    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\n",
    "        alpha = random.choice(param_grid['alpha'])\n",
    "        gamma = random.choice(param_grid['gamma'])\n",
    "        epsilon = random.choice(param_grid['epsilon'])\n",
    "        min_epsilon = random.choice(param_grid['decay_rate'])\n",
    "        decay_rate = random.choice(param_grid['decay_rate'])\n",
    "        n_episodes = random.choice(param_grid['n_episodes'])\n",
    "\n",
    "        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\n",
    "        cumulative_pred_reward = evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards)\n",
    "\n",
    "        if cumulative_pred_reward > best_cumulative_pred_reward:\n",
    "            best_cumulative_pred_reward = cumulative_pred_reward\n",
    "            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\n",
    "\n",
    "        print(f\"Iteration cumulative predicted reward: {cumulative_pred_reward:.2f}%, Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\n",
    "\n",
    "    return best_params, best_cumulative_pred_reward\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'gamma': [0.75, 0.85, 0.9, 0.95, 0.99],\n",
    "    'epsilon': [1.0, 0.5, 0.1, 0.05, 0.01, 0.005],\n",
    "    'min_epsilon': [0.05, 0.01, 0.005, 0.001, 0.001],\n",
    "    'decay_rate': [0.95, 0.99, 0.995, 0.997, 0.999],\n",
    "    'n_episodes': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 16000, 18000, 20000, 22000, 24000, 26000, 28000, 30000]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "best_params, best_cumulative_pred_reward = random_search_prediction_efficiency(50, param_grid, n_states, n_actions, rewards, train_data)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
