{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas import DataFrame\n",
    "import pickle, random\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0 = pd.read_csv('../spreadsheets/rlhf_1064.csv') # 0.005, 0.75, 0.1, 0.95, 0.999, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_159nlp.csv') # second Best 0.01, 0.85, 0.01, 0.95, 0.95, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_07rl.csv') # Best\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_1072.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/shufled_rlhf_11rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_12rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_15rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_154nlp_balanced.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_19rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_24rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_23rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_25rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_26rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_157nlp.csv') # 0.7, 0.95, 0.5, 0.999, 0.99, 16000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_157nlpgate.csv') # 0.25, 0.95, 0.01, 0.997, 0.999, 14000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_29rl.csv') # 0.9, 0.9, 0.005, 0.95, 0.999, 10000,\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_30rl.csv') # 0.005, 0.75, 0.1, 0.95, 0.999, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_32rl.csv') # 0.01, 0.85, 0.01, 0.95, 0.95, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_33rl.csv')# 0.05, 0.85, 0.01, 0.997, 0.95, 4000\n",
    "# df0 = pd.read_csv('../spreadsheets/rlhf_small_36rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_39rl.csv')# 0.05, 0.85, 0.01, 0.997, 0.95, 4000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_154nlp_refined.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_27rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_43rl.csv') # 0.001, 0.99, 1.0, 0.95, 0.99, 10000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_1064_2.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_154nlp_refined.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_46rl_balanced.csv')\n",
    "df0 = pd.read_csv('../spreadsheets/rlhf_large_71rl_refined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'ema-26', 'ema-12', 'low', 'mean-grad-hist', 'close',\n",
       "       'volume', 'sma-05', 'sma-07', 'sma-25', 'long_jcrosk', 'short_kdj',\n",
       "       'sma-compare', 'ask', 'bid', 'is_short', 'nlpreds', 'action',\n",
       "       'predicted_action', 'reward', 'refined-action'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['action'] = df0['action'].replace('go_long', 'do_nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>ema-26</th>\n",
       "      <th>ema-12</th>\n",
       "      <th>low</th>\n",
       "      <th>mean-grad-hist</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>...</th>\n",
       "      <th>short_kdj</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>is_short</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>action</th>\n",
       "      <th>predicted_action</th>\n",
       "      <th>reward</th>\n",
       "      <th>refined-action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4370</td>\n",
       "      <td>0.4489</td>\n",
       "      <td>0.433521</td>\n",
       "      <td>0.427565</td>\n",
       "      <td>0.4368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.444</td>\n",
       "      <td>3.393058e+06</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.437086</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.710009e+06</td>\n",
       "      <td>1.683049e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>go_short</td>\n",
       "      <td>-17.884543</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.7100</td>\n",
       "      <td>98.6400</td>\n",
       "      <td>95.003382</td>\n",
       "      <td>92.862650</td>\n",
       "      <td>95.6800</td>\n",
       "      <td>1</td>\n",
       "      <td>97.240</td>\n",
       "      <td>1.588277e+05</td>\n",
       "      <td>96.62200</td>\n",
       "      <td>96.552857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.004356e+04</td>\n",
       "      <td>7.878414e+04</td>\n",
       "      <td>1</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>2.516894</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.9410</td>\n",
       "      <td>2.9460</td>\n",
       "      <td>2.954570</td>\n",
       "      <td>2.978340</td>\n",
       "      <td>2.8000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.848</td>\n",
       "      <td>4.523280e+05</td>\n",
       "      <td>2.93440</td>\n",
       "      <td>2.944429</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.225307e+05</td>\n",
       "      <td>2.297973e+05</td>\n",
       "      <td>1</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>19.229036</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3251.1000</td>\n",
       "      <td>3321.7300</td>\n",
       "      <td>3220.306179</td>\n",
       "      <td>3198.109119</td>\n",
       "      <td>3243.2000</td>\n",
       "      <td>0</td>\n",
       "      <td>3302.170</td>\n",
       "      <td>3.151710e+05</td>\n",
       "      <td>3249.67600</td>\n",
       "      <td>3233.615714</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.588136e+05</td>\n",
       "      <td>1.563574e+05</td>\n",
       "      <td>1</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>4.416351</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.6892</td>\n",
       "      <td>1.7862</td>\n",
       "      <td>1.638430</td>\n",
       "      <td>1.606013</td>\n",
       "      <td>1.6824</td>\n",
       "      <td>0</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.163686e+07</td>\n",
       "      <td>1.64886</td>\n",
       "      <td>1.636814</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.921292e+06</td>\n",
       "      <td>5.715570e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        open       high       ema-26       ema-12        low  mean-grad-hist  \\\n",
       "0     0.4370     0.4489     0.433521     0.427565     0.4368               0   \n",
       "1    95.7100    98.6400    95.003382    92.862650    95.6800               1   \n",
       "2     2.9410     2.9460     2.954570     2.978340     2.8000               1   \n",
       "3  3251.1000  3321.7300  3220.306179  3198.109119  3243.2000               0   \n",
       "4     1.6892     1.7862     1.638430     1.606013     1.6824               0   \n",
       "\n",
       "      close        volume      sma-05       sma-07  ...  short_kdj  \\\n",
       "0     0.444  3.393058e+06     0.43750     0.437086  ...          0   \n",
       "1    97.240  1.588277e+05    96.62200    96.552857  ...          0   \n",
       "2     2.848  4.523280e+05     2.93440     2.944429  ...          0   \n",
       "3  3302.170  3.151710e+05  3249.67600  3233.615714  ...          0   \n",
       "4     1.750  1.163686e+07     1.64886     1.636814  ...          0   \n",
       "\n",
       "   sma-compare           ask           bid  is_short  nlpreds      action  \\\n",
       "0            0  1.710009e+06  1.683049e+06         1  go_long  do_nothing   \n",
       "1            0  8.004356e+04  7.878414e+04         1  go_long     go_long   \n",
       "2            1  2.225307e+05  2.297973e+05         1  go_long     go_long   \n",
       "3            0  1.588136e+05  1.563574e+05         1  go_long     go_long   \n",
       "4            0  5.921292e+06  5.715570e+06         0  go_long  do_nothing   \n",
       "\n",
       "  predicted_action     reward refined-action  \n",
       "0         go_short -17.884543     do_nothing  \n",
       "1         go_short   2.516894        go_long  \n",
       "2       do_nothing  19.229036        go_long  \n",
       "3         go_short   4.416351        go_long  \n",
       "4          go_long  -0.000283     do_nothing  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['refined-action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "do_nothing    235\n",
       "go_long       194\n",
       "go_short       58\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df: DataFrame) -> DataFrame:\n",
    "    train_data = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        col_name = col.split(' ')[0]\n",
    "        train_data[f'{col_name}'] = df[col]\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf0 = pd.DataFrame()\n",
    "train_data = prep_data(df0) if newdf0.empty else prep_data(newdf0)\n",
    "#train_data = prep_data(newdf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>ema-26</th>\n",
       "      <th>ema-12</th>\n",
       "      <th>low</th>\n",
       "      <th>mean-grad-hist</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>...</th>\n",
       "      <th>short_kdj</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>is_short</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>action</th>\n",
       "      <th>predicted_action</th>\n",
       "      <th>reward</th>\n",
       "      <th>refined-action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.437</td>\n",
       "      <td>0.4489</td>\n",
       "      <td>0.433521</td>\n",
       "      <td>0.427565</td>\n",
       "      <td>0.4368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.444</td>\n",
       "      <td>3393057.500</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.437086</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.710009e+06</td>\n",
       "      <td>1.683049e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>go_long</td>\n",
       "      <td>do_nothing</td>\n",
       "      <td>go_short</td>\n",
       "      <td>-17.884543</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.710</td>\n",
       "      <td>98.6400</td>\n",
       "      <td>95.003382</td>\n",
       "      <td>92.862650</td>\n",
       "      <td>95.6800</td>\n",
       "      <td>1</td>\n",
       "      <td>97.240</td>\n",
       "      <td>158827.699</td>\n",
       "      <td>96.6220</td>\n",
       "      <td>96.552857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.004356e+04</td>\n",
       "      <td>7.878414e+04</td>\n",
       "      <td>1</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>2.516894</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     open     high     ema-26     ema-12      low  mean-grad-hist   close  \\\n",
       "0   0.437   0.4489   0.433521   0.427565   0.4368               0   0.444   \n",
       "1  95.710  98.6400  95.003382  92.862650  95.6800               1  97.240   \n",
       "\n",
       "        volume   sma-05     sma-07  ...  short_kdj  sma-compare           ask  \\\n",
       "0  3393057.500   0.4375   0.437086  ...          0            0  1.710009e+06   \n",
       "1   158827.699  96.6220  96.552857  ...          0            0  8.004356e+04   \n",
       "\n",
       "            bid  is_short  nlpreds      action predicted_action     reward  \\\n",
       "0  1.683049e+06         1  go_long  do_nothing         go_short -17.884543   \n",
       "1  7.878414e+04         1  go_long     go_long         go_short   2.516894   \n",
       "\n",
       "  refined-action  \n",
       "0     do_nothing  \n",
       "1        go_long  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode actions into numerical values\n",
    "action_mapping = {\"go_long\": 0, \"go_short\": 1, \"do_nothing\": 2}\n",
    "action_col = 'action' if newdf0.empty else 'refined-action'\n",
    "train_data[\"action_num\"] = train_data[f\"{action_col}\"].map(action_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'ema-26', 'ema-12', 'low', 'mean-grad-hist', 'close',\n",
       "       'volume', 'sma-05', 'sma-07', 'sma-25', 'long_jcrosk', 'short_kdj',\n",
       "       'sma-compare', 'ask', 'bid', 'is_short', 'nlpreds', 'action',\n",
       "       'predicted_action', 'reward', 'refined-action', 'action_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL parameters\n",
    "states = train_data[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values  # Include binary_state\n",
    "actions = list(action_mapping.values())  # Action space\n",
    "rewards = train_data[\"reward\"].values  # Rewards\n",
    "n_states = states.shape[0]\n",
    "n_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.95, 1.0, 0.99, 0.99, 10000]\n"
     ]
    }
   ],
   "source": [
    "list_1 = [\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "]\n",
    "\n",
    "list_2 = [\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "]\n",
    "\n",
    "# Combine the lists and remove duplicates\n",
    "combined_set = {tuple(sublist) for sublist in list_1 + list_2}\n",
    "\n",
    "# Convert the set back to a list of lists\n",
    "combined_list = [list(sublist) for sublist in combined_set]\n",
    "\n",
    "# Print the combined list\n",
    "for sublist in combined_list:\n",
    "    print(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameters = [\n",
    "    [0.001, 0.99, 1.0, 0.95, 0.99, 10000],\n",
    "    [1, 0.9, 1.0, 0.99, 0.99, 11000],\n",
    "    [0.1, 0.9, 0.1, 0.99, 0.995, 4000],\n",
    "    [0.005, 0.75, 0.1, 0.95, 0.999, 12000],\n",
    "    [0.001, 0.75, 1.0, 0.99, 0.99, 30000],\n",
    "    [1, 0.75, 0.005, 0.95, 0.95, 22000],\n",
    "    [0.01, 0.99, 1.0, 0.95, 0.99, 16000],\n",
    "    [0.7, 0.99, 1.0, 0.95, 0.997, 8000],\n",
    "    [0.01, 0.95, 1.0, 0.997, 0.995, 26000],\n",
    "    [0.25, 0.95, 0.01, 0.997, 0.999, 14000],\n",
    "    [0.5, 0.85, 0.5, 0.997, 0.997, 14000],\n",
    "    [0.01, 0.85, 0.01, 0.95, 0.95, 12000],\n",
    "    [0.9, 0.99, 0.5, 0.995, 0.95, 12000],\n",
    "    [0.05, 0.9, 0.5, 0.95, 0.999, 4000],\n",
    "    [0.05, 0.99, 0.5, 0.99, 0.997, 6000],\n",
    "    [1, 0.75, 0.05, 0.999, 0.999, 10000],\n",
    "    [0.9, 0.95, 1.0, 0.99, 0.99, 8000],\n",
    "    [0.25, 0.75, 0.01, 0.995, 0.999, 20000],\n",
    "    [0.3, 0.75, 1.0, 0.995, 0.99, 10000],\n",
    "    [1, 0.9, 1.0, 0.999, 0.999, 10000],\n",
    "    [0.7, 0.75, 1.0, 0.97, 0.999, 28000],\n",
    "    [0.05, 0.95, 1.0, 0.999, 0.995, 12000],\n",
    "    [0.7, 0.95, 0.5, 0.999, 0.99, 16000],\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "    [0.25, 0.99, 0.01, 0.997, 0.99, 8000],\n",
    "    [1, 0.95, 0.1, 0.96, 0.96, 12000],\n",
    "    [0.9, 0.9, 0.005, 0.95, 0.999, 10000],\n",
    "    [0.05, 0.85, 0.01, 0.997, 0.95, 4000],\n",
    "    [0.01, 0.9, 0.5, 0.999, 0.999, 1500],\n",
    "    [1, 0.85, 1.0, 0.95, 0.997, 21000],\n",
    "    [0.7, 0.9, 0.05, 0.95, 0.95, 20000]\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [0.001, 0.99, 1.0, 0.95, 0.99, 10000])\n",
      "(1, [1, 0.9, 1.0, 0.99, 0.99, 11000])\n",
      "(2, [0.1, 0.9, 0.1, 0.99, 0.995, 4000])\n",
      "(3, [0.005, 0.75, 0.1, 0.95, 0.999, 12000])\n",
      "(4, [0.001, 0.75, 1.0, 0.99, 0.99, 30000])\n",
      "(5, [1, 0.75, 0.005, 0.95, 0.95, 22000])\n",
      "(6, [0.01, 0.99, 1.0, 0.95, 0.99, 16000])\n",
      "(7, [0.7, 0.99, 1.0, 0.95, 0.997, 8000])\n",
      "(8, [0.01, 0.95, 1.0, 0.997, 0.995, 26000])\n",
      "(9, [0.25, 0.95, 0.01, 0.997, 0.999, 14000])\n",
      "(10, [0.5, 0.85, 0.5, 0.997, 0.997, 14000])\n",
      "(11, [0.01, 0.85, 0.01, 0.95, 0.95, 12000])\n",
      "(12, [0.9, 0.99, 0.5, 0.995, 0.95, 12000])\n",
      "(13, [0.05, 0.9, 0.5, 0.95, 0.999, 4000])\n",
      "(14, [0.05, 0.99, 0.5, 0.99, 0.997, 6000])\n",
      "(15, [1, 0.75, 0.05, 0.999, 0.999, 10000])\n",
      "(16, [0.9, 0.95, 1.0, 0.99, 0.99, 8000])\n",
      "(17, [0.25, 0.75, 0.01, 0.995, 0.999, 20000])\n",
      "(18, [0.3, 0.75, 1.0, 0.995, 0.99, 10000])\n",
      "(19, [1, 0.9, 1.0, 0.999, 0.999, 10000])\n",
      "(20, [0.7, 0.75, 1.0, 0.97, 0.999, 28000])\n",
      "(21, [0.05, 0.95, 1.0, 0.999, 0.995, 12000])\n",
      "(22, [0.7, 0.95, 0.5, 0.999, 0.99, 16000])\n",
      "(23, [0.25, 0.95, 1.0, 0.99, 0.99, 10000])\n",
      "(24, [0.25, 0.99, 0.01, 0.997, 0.99, 8000])\n",
      "(25, [1, 0.95, 0.1, 0.96, 0.96, 12000])\n",
      "(26, [0.9, 0.9, 0.005, 0.95, 0.999, 10000])\n",
      "(27, [0.05, 0.85, 0.01, 0.997, 0.95, 4000])\n",
      "(28, [0.01, 0.9, 0.5, 0.999, 0.999, 1500])\n",
      "(29, [1, 0.85, 1.0, 0.95, 0.997, 21000])\n",
      "(30, [0.7, 0.9, 0.05, 0.95, 0.95, 20000])\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(Hyperparameters):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3, 0.75, 1.0, 0.995, 0.99, 10000\n",
    "'''\n",
    "alpha = 0.7\n",
    "gamma = 0.75\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.97\n",
    "decay_rate = 0.97\n",
    "n_episodes = 28000\n",
    "n_states = states.shape[0]  # Number of states\n",
    "n_actions = len(actions)  # Number of actions\n",
    "'''\n",
    "alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes = Hyperparameters[4] # Hyperparameters[30] # Hyperparameters[22] #Hyperparameters[9] # Hyperparameters[21]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_index_mapping(df):\n",
    "    state_to_index = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        state = (row['sma-05'], row['sma-07'], row['sma-25'], row['sma-compare'], row['is_short'])\n",
    "        state_to_index[state] = idx\n",
    "    return state_to_index\n",
    "\n",
    "# Assuming 'df' is your dataframe used during training\n",
    "state_to_index = create_state_index_mapping(train_data)\n",
    "\n",
    "# Save the state_to_index dictionary for later use\n",
    "np.save('small_state_to_index.npy', state_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to choose an action using epsilon-greedy\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, n_actions)  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit: best known action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   0%|          | 62/30000 [00:00<00:48, 618.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/30000 - Total Reward: -1816.118925330002, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   3%|▎         | 898/30000 [00:01<00:43, 665.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800/30000 - Total Reward: -1190.499934240003, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   6%|▌         | 1714/30000 [00:02<00:41, 683.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600/30000 - Total Reward: -1605.6476926900002, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   8%|▊         | 2487/30000 [00:03<00:40, 684.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400/30000 - Total Reward: -2324.041898079998, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  11%|█         | 3341/30000 [00:04<00:35, 741.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200/30000 - Total Reward: -4100.7501562100015, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  14%|█▎        | 4122/30000 [00:06<00:37, 689.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000/30000 - Total Reward: -5184.274198959999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  16%|█▋        | 4885/30000 [00:07<00:38, 652.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4800/30000 - Total Reward: -3121.657200529999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  19%|█▉        | 5668/30000 [00:08<00:34, 697.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5600/30000 - Total Reward: -656.21329215, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  22%|██▏       | 6493/30000 [00:09<00:33, 692.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6400/30000 - Total Reward: -1868.70538526, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  24%|██▍       | 7278/30000 [00:10<00:33, 682.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7200/30000 - Total Reward: -3433.25961475, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  27%|██▋       | 8126/30000 [00:12<00:33, 660.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8000/30000 - Total Reward: -2661.299994369998, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  30%|██▉       | 8899/30000 [00:13<00:31, 678.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8800/30000 - Total Reward: -2226.449752389999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  32%|███▏      | 9727/30000 [00:14<00:30, 661.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9600/30000 - Total Reward: -2588.6728600099977, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  35%|███▌      | 10515/30000 [00:15<00:28, 691.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10400/30000 - Total Reward: -3046.321511929999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  38%|███▊      | 11339/30000 [00:16<00:25, 734.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11200/30000 - Total Reward: -3527.614069460001, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  40%|████      | 12092/30000 [00:17<00:26, 677.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12000/30000 - Total Reward: -4504.39394988, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  43%|████▎     | 12929/30000 [00:19<00:25, 680.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12800/30000 - Total Reward: -3413.6268529299996, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  46%|████▌     | 13713/30000 [00:20<00:22, 726.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13600/30000 - Total Reward: -5136.572212499999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  48%|████▊     | 14524/30000 [00:21<00:21, 703.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14400/30000 - Total Reward: -3294.4567535799993, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  51%|█████     | 15290/30000 [00:22<00:23, 616.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15200/30000 - Total Reward: -1137.931653800001, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  54%|█████▎    | 16104/30000 [00:23<00:20, 687.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16000/30000 - Total Reward: 179.96170312000103, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  56%|█████▋    | 16877/30000 [00:24<00:18, 700.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16800/30000 - Total Reward: -1702.7233792600002, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  59%|█████▉    | 17702/30000 [00:26<00:18, 653.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17600/30000 - Total Reward: -2805.2560532499974, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  62%|██████▏   | 18490/30000 [00:27<00:16, 709.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18400/30000 - Total Reward: -1751.3771162500004, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  64%|██████▍   | 19320/30000 [00:28<00:18, 579.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19200/30000 - Total Reward: -4311.975542989999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  67%|██████▋   | 20080/30000 [00:29<00:14, 694.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000/30000 - Total Reward: -504.85151041999995, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  70%|██████▉   | 20928/30000 [00:31<00:13, 661.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20800/30000 - Total Reward: -3551.0492038500006, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  72%|███████▏  | 21731/30000 [00:32<00:12, 677.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21600/30000 - Total Reward: -4972.998737429999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  75%|███████▌  | 22573/30000 [00:33<00:11, 652.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22400/30000 - Total Reward: -2550.979501119998, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  78%|███████▊  | 23292/30000 [00:34<00:10, 619.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23200/30000 - Total Reward: -197.40362294999963, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  81%|████████  | 24163/30000 [00:36<00:09, 586.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24000/30000 - Total Reward: -4638.518100509999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  83%|████████▎ | 24877/30000 [00:37<00:08, 615.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24800/30000 - Total Reward: -1285.6717980900023, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  86%|████████▌ | 25728/30000 [00:38<00:06, 651.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25600/30000 - Total Reward: -1532.938294220002, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  88%|████████▊ | 26481/30000 [00:39<00:05, 660.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26400/30000 - Total Reward: -4638.518100509999, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  91%|█████████ | 27341/30000 [00:40<00:03, 680.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27200/30000 - Total Reward: -1162.2516025900006, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  94%|█████████▎| 28065/30000 [00:41<00:02, 671.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28000/30000 - Total Reward: -2500.1739277699985, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  96%|█████████▋| 28912/30000 [00:43<00:01, 658.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28800/30000 - Total Reward: 132.27198298000192, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  99%|█████████▉| 29759/30000 [00:44<00:00, 605.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29600/30000 - Total Reward: 209.02523003000132, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...: 100%|██████████| 30000/30000 [00:44<00:00, 668.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simplified environment function\n",
    "def environment_step(current_state, action):\n",
    "    \"\"\"\n",
    "    Simulates the environment's response to an action.\n",
    "    \n",
    "    Args:\n",
    "        current_state (int): The current state of the environment.\n",
    "        action (int): The action taken by the agent.\n",
    "    \n",
    "    Returns:\n",
    "        next_state (int): The next state after taking the action.\n",
    "        reward (float): The reward received after taking the action.\n",
    "    \"\"\"\n",
    "    # Define the environment logic here\n",
    "    next_state = current_state + 1  # Example: Move to the next state\n",
    "    reward = rewards[next_state]    # Example: Reward is based on the next state\n",
    "    \n",
    "    return next_state, reward\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize a list to store rewards per episode\n",
    "rewards_per_episode = []\n",
    "\n",
    "# Training loop\n",
    "for episode in tqdm(range(n_episodes), desc=\"evaluating results per episode ...\"):\n",
    "    current_state = np.random.randint(0, n_states)  # Random initial state\n",
    "    total_reward = 0  # Initialize total reward for the current episode\n",
    "\n",
    "    while current_state < n_states - 1:\n",
    "        action = choose_action(current_state, epsilon)\n",
    "        \n",
    "        # Use the environment function to get the next state and reward\n",
    "        next_state, reward = environment_step(current_state, action)\n",
    "\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        q_table[current_state, action] += alpha * (\n",
    "            reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "        )\n",
    "        \n",
    "        total_reward += reward  # Accumulate reward for the current episode\n",
    "        current_state = next_state  # Move to next state\n",
    "\n",
    "    rewards_per_episode.append(total_reward)  # Store the total reward for the current episode\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if episode % 800 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Episode {episode}/{n_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "# Example: Save the Q-table\n",
    "np.save(\"small_q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_q_table(file_path):\n",
    "    return np.load(file_path)\n",
    "\n",
    "def load_state_index_mapping(file_path):\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "loaded_mapping = load_state_index_mapping(file_path=\"small_state_to_index.npy\")\n",
    "loaded_qtable = load_q_table(file_path=\"small_q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_state(sma_05: float, sma_07: float, sma_25: float, sma_compare: int, is_short: int):\n",
    "    state = np.array([[sma_05, sma_07, sma_25, sma_compare, is_short]])\n",
    "    if not np.all(np.isfinite(state)):\n",
    "        state = np.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_action(state, q_table, state_to_index, action_mapping, default_action: str = None):\n",
    "    state_tuple = tuple(state.flatten())\n",
    "\n",
    "    state_index = state_to_index.get(state_tuple, -1)\n",
    "\n",
    "    if not state_index == -1:\n",
    "        try:\n",
    "            q_values = q_table[state_index]\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            #return default_action\n",
    "    else:\n",
    "        state_tuples = list(state_to_index.keys())\n",
    "        kdtree = KDTree(state_tuples)\n",
    "        distance, index = kdtree.query(state.flatten())\n",
    "        nearest_state_tuple = state_tuples[index]\n",
    "        new_state_index = state_to_index[nearest_state_tuple]\n",
    "        q_values = loaded_qtable[new_state_index]\n",
    "    \n",
    "    #q_values = q_table[state_index]\n",
    "    best_action_index = np.argmax(q_values)\n",
    "    action = [action for action, index in action_mapping.items() if index == best_action_index][0]\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "with open(\"small_q_table.npy\", \"rb\") as f:\n",
    "    q_table = load_q_table(\"small_q_table.npy\")\n",
    "\n",
    "with open(\"small_state_to_index.npy\", \"rb\") as f:\n",
    "    state_to_index = load_state_index_mapping(\"small_state_to_index.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values\n",
    "state_1 = list(X[-1:].flatten()) # sample: [[0.87024    0.85277143 0.779504   0.         1.        ]]\n",
    "\n",
    "state = prep_state(*state_1)\n",
    "action_mapping = {\"go_long\": 0, \"go_short\": 1, \"do_nothing\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted action for the state is: go_long\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predicted_action = predict_action(state, q_table, state_to_index, action_mapping)\n",
    "    print(f\"The predicted action for the state is: {predicted_action}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"The state is not found in the state index mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict an entire range\n",
    "for idx, row in train_data.iterrows():\n",
    "    state = row[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values\n",
    "    action = predict_action(state, q_table, state_to_index, action_mapping)\n",
    "    train_data.loc[idx, \"predicted_action\"] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_data['predicted_action'].nunique() < 2):\n",
    "    raise ValueError(\"Model predictions are invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_action\n",
       "do_nothing    176\n",
       "go_long       161\n",
       "go_short      150\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['predicted_action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_data[(train_data['nlpreds'] == 'go_long') & (train_data['reward'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_short\n",
       "1    113\n",
       "0     75\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['is_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "go_long       188\n",
       "go_short       56\n",
       "do_nothing      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df0[(df0['reward'] > 0)]\n",
    "s['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed state 0/486\n",
      "Current Predicted Reward: 0\n",
      "Current Actual Reward: 2.51689387\n",
      "Processed state 100/486\n",
      "Current Predicted Reward: 710.10646613\n",
      "Current Actual Reward: 2004.70264424\n",
      "Processed state 200/486\n",
      "Current Predicted Reward: 1607.7180059999994\n",
      "Current Actual Reward: 4357.68791454\n",
      "Processed state 300/486\n",
      "Current Predicted Reward: 1424.5412338599992\n",
      "Current Actual Reward: 4144.7320186000015\n",
      "Processed state 400/486\n",
      "Current Predicted Reward: 305.224771359999\n",
      "Current Actual Reward: 1750.4896360500013\n",
      "Cumulative Predicted Reward: -273.8382271400009\n",
      "Cumulative Actual Reward: 211.5421239000021\n",
      "Prediction Efficiency: -229.45%\n"
     ]
    }
   ],
   "source": [
    "# Performance measures\n",
    "# Initialize cumulative rewards\n",
    "cumulative_predicted_reward = 0\n",
    "cumulative_actual_reward = 0\n",
    "\n",
    "# Iterate through states to calculate rewards\n",
    "for state_index in range(n_states - 1):\n",
    "    # Predicted action from Q-table\n",
    "    predicted_action = np.argmax(q_table[state_index])  # Best action for the current state\n",
    "    # Actual action from the ground truth\n",
    "    actual_action = train_data[\"action_num\"].iloc[state_index]\n",
    "\n",
    "    # Get reward for predicted action only if it matches the actual action\n",
    "    if predicted_action == actual_action:\n",
    "        predicted_reward = rewards[state_index + 1]  # Reward for the correct prediction\n",
    "        cumulative_predicted_reward += predicted_reward\n",
    "\n",
    "    # Get actual reward for the ground truth action\n",
    "    actual_reward = rewards[state_index + 1]\n",
    "    cumulative_actual_reward += actual_reward\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if state_index % 100 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Processed state {state_index}/{n_states - 1}\")\n",
    "        print(f\"Current Predicted Reward: {cumulative_predicted_reward}\")\n",
    "        print(f\"Current Actual Reward: {cumulative_actual_reward}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cumulative Predicted Reward: {cumulative_predicted_reward}\")\n",
    "print(f\"Cumulative Actual Reward: {cumulative_actual_reward}\")\n",
    "\n",
    "# Optionally calculate efficiency\n",
    "efficiency = (\n",
    "    ((cumulative_predicted_reward - cumulative_actual_reward) / abs(cumulative_actual_reward)) * 100\n",
    "    if cumulative_actual_reward != 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "print(f\"Prediction Efficiency: {efficiency:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.81%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "correct_predictions = 0\n",
    "for state_index in range(n_states):\n",
    "    predicted_action = np.argmax(q_table[state_index])  # Predicted action\n",
    "    actual_action = train_data[\"action_num\"].iloc[state_index]  # Actual action\n",
    "    if predicted_action == actual_action:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / n_states\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 62  60  54]\n",
      " [ 29  25  20]\n",
      " [ 70  65 102]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "y_true = train_data[\"action_num\"]  # Actual actions\n",
    "y_pred = [np.argmax(q_table[state_index]) for state_index in range(n_states)]  # Predicted actions\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_reward(action: str, is_short: int):\n",
    "    m = train_data[(train_data['predicted_action'] == f'{action}') & (train_data['is_short'] == is_short)]\n",
    "    counts = m['is_short'].value_counts()\n",
    "    total_reward = m['reward'].cumsum()[-1:].values[0]\n",
    "    wins = len(m[m['reward'] > 0])\n",
    "    losses = len(m[m['reward'] <= 0])\n",
    "    return {\n",
    "        'counts': counts.get(is_short),\n",
    "        'total reward': total_reward,\n",
    "        'winrate': f'{wins * 100 / (losses + wins):.2f}%',\n",
    "        'per trade profit': m[m['reward'] > 0]['reward'].sum() / wins,\n",
    "        'per trade loss': m[m['reward'] <= 0]['reward'].sum() / losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counts': 84,\n",
       " 'total reward': -35.95439132999959,\n",
       " 'winrate': '50.00%',\n",
       " 'per trade profit': 21.60254193690476,\n",
       " 'per trade loss': -22.458598873333333}"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_reward('do_nothing', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go_long 0: {'counts': 72, 'total reward': -27.2480144999999, 'winrate': '48.61%', 'per trade profit': 22.286798442, 'per trade loss': -21.81853945864865}\n",
      "go_long 1: {'counts': 89, 'total reward': 554.6568654999996, 'winrate': '62.92%', 'per trade profit': 22.932608558392854, 'per trade loss': -22.108157993030307}\n",
      "go_short 0: {'counts': 75, 'total reward': -27.076897609999815, 'winrate': '50.67%', 'per trade profit': 21.21856205026316, 'per trade loss': -22.523844743783783}\n",
      "go_short 1: {'counts': 75, 'total reward': 384.95088403000017, 'winrate': '62.67%', 'per trade profit': 21.466384040000005, 'per trade loss': -22.284613066071426}\n",
      "do_nothing 0: {'counts': 92, 'total reward': -655.67086535, 'winrate': '34.78%', 'per trade profit': 21.966035208125, 'per trade loss': -22.6430665335}\n",
      "do_nothing 1: {'counts': 84, 'total reward': -35.95439132999959, 'winrate': '50.00%', 'per trade profit': 21.60254193690476, 'per trade loss': -22.458598873333333}\n"
     ]
    }
   ],
   "source": [
    "dirs = [0,1]\n",
    "for action in action_mapping.keys():\n",
    "    for is_short in dirs:\n",
    "        try:\n",
    "            print(f'{action} {is_short}: {action_reward(action, is_short)}')\n",
    "        except IndexError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_action\n",
      "do_nothing    176\n",
      "go_long       161\n",
      "go_short      150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['predicted_action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef correct_action(row):\\n    if row[\\'predicted_action\\'] == \\'go_long\\' and row[\\'is_short\\'] == 1 and row[\\'reward\\'] > 0:\\n        return \\'go_short\\'\\n    if row[\\'predicted_action\\'] == \\'go_short\\' and row[\\'is_short\\'] == 0 and row[\\'reward\\'] > 0:\\n        return \\'go_long\\'\\n    if row[\\'predicted_action\\'] == \\'do_nothing\\' and row[\\'is_short\\'] == 1 and row[\\'reward\\'] > 0:\\n        return \\'go_short\\'\\n    if row[\\'predicted_action\\'] == \\'do_nothing\\' and row[\\'is_short\\'] == 0 and row[\\'reward\\'] > 0:\\n        return \\'go_long\\'\\n    return row[\\'action\\']\\n\\ndef refiner_action(version: str, data: DataFrame = None) -> DataFrame:\\n    data[\\'refined-action\\'] = data.apply(lambda x: correct_action(x), axis=1)\\n    # Validation: Ensure we\\'re fixing the 751 misclassified entries\\n    misclassified = data[\\n        (data[\\'predicted_action\\'] == \\'go_long\\') & \\n        (data[\\'refined-action\\'] == \\'go_short\\')\\n    ]\\n    # Add this after applying refined-action\\n    \\n    confusion_matrix = pd.crosstab(\\n        data[\\'refined-action\\'], \\n        data[\\'predicted_action\\'],  # Assuming you have ground truth column\\n        rownames=[\\'refined\\'],\\n        colnames=[\\'predicted\\']\\n    )\\n\\n    print(\"Updated Confusion Matrix:\")\\n    print(confusion_matrix) \\n    print(f\"Corrected {len(misclassified)} go_long->go_short misclassifications\")\\n    \\n    filename = f\\'../spreadsheets/rlhf_large_{version}_refined.csv\\'\\n    data.to_csv(filename, index=False)\\n    return data\\n    '"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create corrected action\n",
    "'''\n",
    "def correct_action(row):\n",
    "    if row['predicted_action'] == 'go_long' and row['is_short'] == 1 and row['reward'] > 0:\n",
    "        return 'go_short'\n",
    "    if row['predicted_action'] == 'go_short' and row['is_short'] == 0 and row['reward'] > 0:\n",
    "        return 'go_long'\n",
    "    if row['predicted_action'] == 'do_nothing' and row['is_short'] == 1 and row['reward'] > 0:\n",
    "        return 'go_short'\n",
    "    if row['predicted_action'] == 'do_nothing' and row['is_short'] == 0 and row['reward'] > 0:\n",
    "        return 'go_long'\n",
    "    return row['action']\n",
    "\n",
    "def refiner_action(version: str, data: DataFrame = None) -> DataFrame:\n",
    "    data['refined-action'] = data.apply(lambda x: correct_action(x), axis=1)\n",
    "    # Validation: Ensure we're fixing the 751 misclassified entries\n",
    "    misclassified = data[\n",
    "        (data['predicted_action'] == 'go_long') & \n",
    "        (data['refined-action'] == 'go_short')\n",
    "    ]\n",
    "    # Add this after applying refined-action\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(\n",
    "        data['refined-action'], \n",
    "        data['predicted_action'],  # Assuming you have ground truth column\n",
    "        rownames=['refined'],\n",
    "        colnames=['predicted']\n",
    "    )\n",
    "\n",
    "    print(\"Updated Confusion Matrix:\")\n",
    "    print(confusion_matrix) \n",
    "    print(f\"Corrected {len(misclassified)} go_long->go_short misclassifications\")\n",
    "    \n",
    "    filename = f'../spreadsheets/rlhf_large_{version}_refined.csv'\n",
    "    data.to_csv(filename, index=False)\n",
    "    return data\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_action(row):\n",
    "    if row['predicted_action'] == 'go_long' and row['is_short'] == 1 and row['reward'] > 0:\n",
    "        row['is_short'] = 0  # Change to long position\n",
    "        return 'go_short'\n",
    "    if row['predicted_action'] == 'go_short' and row['is_short'] == 0 and row['reward'] > 0:\n",
    "        row['is_short'] = 1  # Change to short position\n",
    "        return 'go_long'\n",
    "    if row['predicted_action'] == 'do_nothing' and row['is_short'] == 1 and row['reward'] > 0:\n",
    "        row['is_short'] = 0  # Change to long position\n",
    "        return 'go_short'\n",
    "    if row['predicted_action'] == 'do_nothing' and row['is_short'] == 0 and row['reward'] > 0:\n",
    "        row['is_short'] = 1  # Change to short position\n",
    "        return 'go_long'\n",
    "    return row['action']\n",
    "\n",
    "def refiner_action(version: str, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[['refined-action', 'is_short']] = data.apply(lambda x: pd.Series([correct_action(x), x['is_short']]), axis=1)\n",
    "\n",
    "    # Validation: Check if is_short aligns with refined-action\n",
    "    inconsistency = data[\n",
    "        ((data['refined-action'] == 'go_short') & (data['is_short'] == 0)) |\n",
    "        ((data['refined-action'] == 'go_long') & (data['is_short'] == 1))\n",
    "    ]\n",
    "    \n",
    "    print(f\"Number of inconsistent rows: {len(inconsistency)}\")\n",
    "    \n",
    "    # Updated Confusion Matrix\n",
    "    confusion_matrix = pd.crosstab(\n",
    "        data['refined-action'], \n",
    "        data['predicted_action'],\n",
    "        rownames=['refined'],\n",
    "        colnames=['predicted']\n",
    "    )\n",
    "\n",
    "    print(\"Updated Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    filename = f'../spreadsheets/rlhf_large_{version}_refined.csv'\n",
    "    data.to_csv(filename, index=False)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/defi/Desktop/portfolio/projects/python/pipeline_defi/'\n",
    "#new_data = pd.read_csv('../spreadsheets/rlhf_small_154nlp.csv') \n",
    "def refine_file(version: str, file) -> DataFrame:\n",
    "    filename = f'{base_dir}{file}.csv'\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename)\n",
    "    #new_data = prep_data(df0.copy()) if newdf0.empty else prep_data(newdf0.copy())\n",
    "    new_data = prep_data(df)   \n",
    "    print(new_data.columns)\n",
    "    new_train_data = refiner_action(version=version, data=new_data)\n",
    "\n",
    "    #new_data = df0.copy()\n",
    "    print(new_train_data.columns)\n",
    "\n",
    "    new_train_data['nlpreds'] = new_train_data['predicted_action']\n",
    "    #new_data['action'] = new_train_data['refined-action']\n",
    "    return new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf0 = refine_file('70rl', 'lean_df_70rl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(newdf0['predicted_action'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf0[(newdf0['predicted_action'] == 'go_short') & (newdf0['is_short'] == 1)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlpreds\n",
      "go_short      182\n",
      "go_long       165\n",
      "do_nothing    140\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(newdf0['nlpreds'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "refined-action\n",
       "do_nothing    237\n",
       "go_long       176\n",
       "go_short       74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf0['refined-action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action\n",
       "do_nothing    237\n",
       "go_long       176\n",
       "go_short       74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf0['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2964.7709842100003\n"
     ]
    }
   ],
   "source": [
    "print(newdf0[(newdf0['nlpreds'] == 'go_short') & (newdf0['reward'] >  0)]['reward'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106.68553214\n"
     ]
    }
   ],
   "source": [
    "print(train_data[(train_data['nlpreds'] == 'go_short') & (train_data['reward'] >  0)]['reward'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of rewards for each episode\n",
    "#rewards_per_episode = [...]  # Populate this with your actual data\n",
    "\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning with Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "def train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards):\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        current_state = np.random.randint(0, n_states)\n",
    "        total_reward = 0\n",
    "\n",
    "        while current_state < n_states - 1:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "\n",
    "            next_state = current_state + 1  # Adjust based on environment logic\n",
    "            reward = rewards[next_state]\n",
    "\n",
    "            best_next_action = np.argmax(q_table[next_state])\n",
    "            q_table[current_state, action] += alpha * (\n",
    "                reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    return q_table, rewards_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards):\n",
    "    correct_predictions = 0\n",
    "    total_reward = 0\n",
    "    reward_weighted_accuracy = []\n",
    "\n",
    "    for state_index in range(n_states):\n",
    "        predicted_action = np.argmax(q_table[state_index])  # Predicted action\n",
    "        actual_action = train_data[\"action_num\"].iloc[state_index]  # Actual action\n",
    "        reward = rewards[state_index]  # Reward for the action\n",
    "\n",
    "        if predicted_action == actual_action:\n",
    "            correct_predictions += 1\n",
    "            total_reward += reward\n",
    "\n",
    "        accuracy = correct_predictions / (state_index + 1)\n",
    "        reward_weighted_accuracy.append(total_reward / (state_index + 1))\n",
    "\n",
    "        # Optional: Log progress\n",
    "        if state_index % 100 == 0:\n",
    "            print(f\"Processed state {state_index}/{n_states} - Accuracy: {accuracy * 100:.2f}%, Reward-weighted Accuracy: {reward_weighted_accuracy[-1]}\")\n",
    "\n",
    "    final_reward_weighted_accuracy = total_reward / n_states\n",
    "    return final_reward_weighted_accuracy * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards):\n",
    "    # Initialize cumulative rewards\n",
    "    cumulative_predicted_reward = 0\n",
    "    cumulative_actual_reward = 0\n",
    "\n",
    "    # Iterate through states to calculate rewards\n",
    "    for state_index in range(n_states - 1):\n",
    "        # Predicted action from Q-table\n",
    "        predicted_action = np.argmax(q_table[state_index])  # Best action for the current state\n",
    "        # Actual action from the ground truth\n",
    "        actual_action = train_data[\"action_num\"].iloc[state_index]\n",
    "\n",
    "        # Get reward for predicted action only if it matches the actual action\n",
    "        if predicted_action == actual_action:\n",
    "            predicted_reward = rewards[state_index + 1]  # Reward for the correct prediction\n",
    "            cumulative_predicted_reward += predicted_reward\n",
    "\n",
    "        # Get actual reward for the ground truth action\n",
    "        actual_reward = rewards[state_index + 1]\n",
    "        cumulative_actual_reward += actual_reward\n",
    "    return cumulative_predicted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef random_search_reward_weighted(n_iter, param_grid, n_states, n_actions, rewards, train_data):\\n    best_params = None\\n    best_reward_weighted_accuracy = float(\\'-inf\\')\\n\\n    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\\n        alpha = random.choice(param_grid[\\'alpha\\'])\\n        gamma = random.choice(param_grid[\\'gamma\\'])\\n        epsilon = random.choice(param_grid[\\'epsilon\\'])\\n        min_epsilon = random.choice(param_grid[\\'decay_rate\\'])\\n        decay_rate = random.choice(param_grid[\\'decay_rate\\'])\\n        n_episodes = random.choice(param_grid[\\'n_episodes\\'])\\n\\n        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\\n        reward_weighted_accuracy = evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards)\\n\\n        if reward_weighted_accuracy > best_reward_weighted_accuracy:\\n            best_reward_weighted_accuracy = reward_weighted_accuracy\\n            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\\n\\n        print(f\"Iteration Reward-weighted Accuracy: {reward_weighted_accuracy:.2f}%, Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\\n\\n    return best_params, best_reward_weighted_accuracy\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'alpha\\': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\\n    \\'gamma\\': [0.75, 0.85, 0.9, 0.95, 0.99],\\n    \\'epsilon\\': [1.0, 0.5, 0.1, 0.05, 0.01],\\n    \\'min_epsilon\\': [0.05, 0.01, 0.005, 0.001, 0.001],\\n    \\'decay_rate\\': [0.95, 0.96, 0.97, 0.99, 0.995, 0.997, 0.999],\\n    \\'n_episodes\\': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 18000, 21000, 24000, 28000]\\n}\\n\\n# Perform Random Search\\nbest_params, best_reward_weighted_accuracy = random_search_reward_weighted(50, param_grid, n_states, n_actions, rewards, train_data)\\nprint(f\"Best Hyperparameters: {best_params}\")\\nprint(f\"Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\\n'"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def random_search_reward_weighted(n_iter, param_grid, n_states, n_actions, rewards, train_data):\n",
    "    best_params = None\n",
    "    best_reward_weighted_accuracy = float('-inf')\n",
    "\n",
    "    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\n",
    "        alpha = random.choice(param_grid['alpha'])\n",
    "        gamma = random.choice(param_grid['gamma'])\n",
    "        epsilon = random.choice(param_grid['epsilon'])\n",
    "        min_epsilon = random.choice(param_grid['decay_rate'])\n",
    "        decay_rate = random.choice(param_grid['decay_rate'])\n",
    "        n_episodes = random.choice(param_grid['n_episodes'])\n",
    "\n",
    "        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\n",
    "        reward_weighted_accuracy = evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards)\n",
    "\n",
    "        if reward_weighted_accuracy > best_reward_weighted_accuracy:\n",
    "            best_reward_weighted_accuracy = reward_weighted_accuracy\n",
    "            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\n",
    "\n",
    "        print(f\"Iteration Reward-weighted Accuracy: {reward_weighted_accuracy:.2f}%, Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\n",
    "\n",
    "    return best_params, best_reward_weighted_accuracy\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'gamma': [0.75, 0.85, 0.9, 0.95, 0.99],\n",
    "    'epsilon': [1.0, 0.5, 0.1, 0.05, 0.01],\n",
    "    'min_epsilon': [0.05, 0.01, 0.005, 0.001, 0.001],\n",
    "    'decay_rate': [0.95, 0.96, 0.97, 0.99, 0.995, 0.997, 0.999],\n",
    "    'n_episodes': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 18000, 21000, 24000, 28000]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "best_params, best_reward_weighted_accuracy = random_search_reward_weighted(50, param_grid, n_states, n_actions, rewards, train_data)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_data = newdf0\\ntrain_data[\\'action_num\\'] = train_data[f\"refined-action\"].map(action_mapping)\\ndef random_search_prediction_efficiency(n_iter, param_grid, n_states, n_actions, rewards, train_data):\\n    best_params = None\\n    best_cumulative_pred_reward = float(\\'-inf\\')\\n\\n    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\\n        alpha = random.choice(param_grid[\\'alpha\\'])\\n        gamma = random.choice(param_grid[\\'gamma\\'])\\n        epsilon = random.choice(param_grid[\\'epsilon\\'])\\n        min_epsilon = random.choice(param_grid[\\'decay_rate\\'])\\n        decay_rate = random.choice(param_grid[\\'decay_rate\\'])\\n        n_episodes = random.choice(param_grid[\\'n_episodes\\'])\\n\\n        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\\n        cumulative_pred_reward = evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards)\\n\\n        if cumulative_pred_reward > best_cumulative_pred_reward:\\n            best_cumulative_pred_reward = cumulative_pred_reward\\n            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\\n\\n        print(f\"Iteration cumulative predicted reward: {cumulative_pred_reward:.2f}%, Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\\n\\n    return best_params, best_cumulative_pred_reward\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'alpha\\': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\\n    \\'gamma\\': [0.75, 0.85, 0.9, 0.95, 0.99],\\n    \\'epsilon\\': [1.0, 0.5, 0.1, 0.05, 0.01, 0.005],\\n    \\'min_epsilon\\': [0.05, 0.01, 0.005, 0.001, 0.001],\\n    \\'decay_rate\\': [0.95, 0.99, 0.995, 0.997, 0.999],\\n    \\'n_episodes\\': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 16000, 18000, 20000, 22000, 24000, 26000, 28000, 30000]\\n}\\n\\n# Perform Random Search\\nbest_params, best_cumulative_pred_reward = random_search_prediction_efficiency(50, param_grid, n_states, n_actions, rewards, train_data)\\nprint(f\"Best Hyperparameters: {best_params}\")\\nprint(f\"Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\\n'"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_data = newdf0\n",
    "train_data['action_num'] = train_data[f\"refined-action\"].map(action_mapping)\n",
    "def random_search_prediction_efficiency(n_iter, param_grid, n_states, n_actions, rewards, train_data):\n",
    "    best_params = None\n",
    "    best_cumulative_pred_reward = float('-inf')\n",
    "\n",
    "    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\n",
    "        alpha = random.choice(param_grid['alpha'])\n",
    "        gamma = random.choice(param_grid['gamma'])\n",
    "        epsilon = random.choice(param_grid['epsilon'])\n",
    "        min_epsilon = random.choice(param_grid['decay_rate'])\n",
    "        decay_rate = random.choice(param_grid['decay_rate'])\n",
    "        n_episodes = random.choice(param_grid['n_episodes'])\n",
    "\n",
    "        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\n",
    "        cumulative_pred_reward = evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards)\n",
    "\n",
    "        if cumulative_pred_reward > best_cumulative_pred_reward:\n",
    "            best_cumulative_pred_reward = cumulative_pred_reward\n",
    "            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\n",
    "\n",
    "        print(f\"Iteration cumulative predicted reward: {cumulative_pred_reward:.2f}%, Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\n",
    "\n",
    "    return best_params, best_cumulative_pred_reward\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'gamma': [0.75, 0.85, 0.9, 0.95, 0.99],\n",
    "    'epsilon': [1.0, 0.5, 0.1, 0.05, 0.01, 0.005],\n",
    "    'min_epsilon': [0.05, 0.01, 0.005, 0.001, 0.001],\n",
    "    'decay_rate': [0.95, 0.99, 0.995, 0.997, 0.999],\n",
    "    'n_episodes': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 16000, 18000, 20000, 22000, 24000, 26000, 28000, 30000]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "best_params, best_cumulative_pred_reward = random_search_prediction_efficiency(50, param_grid, n_states, n_actions, rewards, train_data)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
