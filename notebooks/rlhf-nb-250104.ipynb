{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas import DataFrame\n",
    "import pickle, random\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0 = pd.read_csv('../spreadsheets/rlhf_1064.csv') # 0.005, 0.75, 0.1, 0.95, 0.999, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_159nlp.csv') # second Best 0.01, 0.85, 0.01, 0.95, 0.95, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_07rl.csv') # Best\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_large_1072.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/shufled_rlhf_11rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_12rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_15rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_154nlp.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_19rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_24rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_23rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_25rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_26rl.csv')\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_157nlp.csv') # 0.7, 0.95, 0.5, 0.999, 0.99, 16000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_157nlpgate.csv') # 0.25, 0.95, 0.01, 0.997, 0.999, 14000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_29rl.csv') # 0.9, 0.9, 0.005, 0.95, 0.999, 10000,\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_30rl.csv') # 0.005, 0.75, 0.1, 0.95, 0.999, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_32rl.csv') # 0.01, 0.85, 0.01, 0.95, 0.95, 12000\n",
    "#df0 = pd.read_csv('../spreadsheets/rlhf_small_33rl.csv')# 0.05, 0.85, 0.01, 0.997, 0.95, 4000\n",
    "df0 = pd.read_csv('../spreadsheets/rlhf_small_36rl.csv')# 0.05, 0.85, 0.01, 0.997, 0.95, 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['action'] = df0['action'].replace('go_long', 'do_nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sma-05 (entry)</th>\n",
       "      <th>sma-07 (entry)</th>\n",
       "      <th>sma-25 (entry)</th>\n",
       "      <th>sma-compare (entry)</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>imit-action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.502460</td>\n",
       "      <td>0.497686</td>\n",
       "      <td>0.488548</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>5.984069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181480</td>\n",
       "      <td>0.180829</td>\n",
       "      <td>0.178788</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.372316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.025259</td>\n",
       "      <td>0.025356</td>\n",
       "      <td>0.026236</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>4.147186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.612120</td>\n",
       "      <td>1.603329</td>\n",
       "      <td>1.571272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>4.228221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14.172400</td>\n",
       "      <td>14.174857</td>\n",
       "      <td>13.866320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>1.886275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sma-05 (entry)  sma-07 (entry)  sma-25 (entry)  \\\n",
       "0           0        0.502460        0.497686        0.488548   \n",
       "1           1        0.181480        0.180829        0.178788   \n",
       "2           2        0.025259        0.025356        0.026236   \n",
       "3           3        1.612120        1.603329        1.571272   \n",
       "4           4       14.172400       14.174857       13.866320   \n",
       "\n",
       "   sma-compare (entry)  is_short    action imit-action  nlpreds    reward  \n",
       "0                    0         0   go_long    go_short  go_long  5.984069  \n",
       "1                    0         1  go_short    go_short  go_long -5.372316  \n",
       "2                    1         0   go_long     go_long  go_long  4.147186  \n",
       "3                    0         0   go_long    go_short  go_long  4.228221  \n",
       "4                    0         0   go_long    go_short  go_long  1.886275  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "go_long     2170\n",
       "go_short     335\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0['imit-action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "go_long     2170\n",
       "go_short     335\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df: DataFrame) -> DataFrame:\n",
    "    train_data = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        col_name = col.split(' ')[0]\n",
    "        train_data[f'{col_name}'] = df[col]\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prep_data(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed:</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>sma-25</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>imit-action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.50246</td>\n",
       "      <td>0.497686</td>\n",
       "      <td>0.488548</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>5.984069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.18148</td>\n",
       "      <td>0.180829</td>\n",
       "      <td>0.178788</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.372316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed:   sma-05    sma-07    sma-25  sma-compare  is_short    action  \\\n",
       "0         0  0.50246  0.497686  0.488548            0         0   go_long   \n",
       "1         1  0.18148  0.180829  0.178788            0         1  go_short   \n",
       "\n",
       "  imit-action  nlpreds    reward  \n",
       "0    go_short  go_long  5.984069  \n",
       "1    go_short  go_long -5.372316  "
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode actions into numerical values\n",
    "action_mapping = {\"go_long\": 0, \"go_short\": 1, \"do_nothing\": 2}\n",
    "train_data[\"action_num\"] = train_data[\"nlpreds\"].map(action_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL parameters\n",
    "states = train_data[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values  # Include binary_state\n",
    "actions = list(action_mapping.values())  # Action space\n",
    "rewards = train_data[\"reward\"].values  # Rewards\n",
    "n_states = states.shape[0]\n",
    "n_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.95, 1.0, 0.99, 0.99, 10000]\n"
     ]
    }
   ],
   "source": [
    "list_1 = [\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "]\n",
    "\n",
    "list_2 = [\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "]\n",
    "\n",
    "# Combine the lists and remove duplicates\n",
    "combined_set = {tuple(sublist) for sublist in list_1 + list_2}\n",
    "\n",
    "# Convert the set back to a list of lists\n",
    "combined_list = [list(sublist) for sublist in combined_set]\n",
    "\n",
    "# Print the combined list\n",
    "for sublist in combined_list:\n",
    "    print(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameters = [\n",
    "    [0.1, 0.9, 0.1, 0.99, 0.995, 4000],\n",
    "    [0.005, 0.75, 0.1, 0.95, 0.999, 12000],\n",
    "    [0.001, 0.75, 1.0, 0.99, 0.99, 30000],\n",
    "    [1, 0.75, 0.005, 0.95, 0.95, 22000],\n",
    "    [0.01, 0.99, 1.0, 0.95, 0.99, 16000],\n",
    "    [0.7, 0.99, 1.0, 0.95, 0.997, 8000],\n",
    "    [0.01, 0.95, 1.0, 0.997, 0.995, 26000],\n",
    "    [0.25, 0.95, 0.01, 0.997, 0.999, 14000],\n",
    "    [0.5, 0.85, 0.5, 0.997, 0.997, 14000],\n",
    "    [0.01, 0.85, 0.01, 0.95, 0.95, 12000],\n",
    "    [0.9, 0.99, 0.5, 0.995, 0.95, 12000],\n",
    "    [0.05, 0.9, 0.5, 0.95, 0.999, 4000],\n",
    "    [0.05, 0.99, 0.5, 0.99, 0.997, 6000],\n",
    "    [1, 0.75, 0.05, 0.999, 0.999, 10000],\n",
    "    [0.9, 0.95, 1.0, 0.99, 0.99, 8000],\n",
    "    [0.25, 0.75, 0.01, 0.995, 0.999, 20000],\n",
    "    [0.3, 0.75, 1.0, 0.995, 0.99, 10000],\n",
    "    [1, 0.9, 1.0, 0.999, 0.999, 10000],\n",
    "    [0.7, 0.75, 1.0, 0.97, 0.999, 28000],\n",
    "    [0.05, 0.95, 1.0, 0.999, 0.995, 12000],\n",
    "    [0.7, 0.95, 0.5, 0.999, 0.99, 16000],\n",
    "    [0.25, 0.95, 1.0, 0.99, 0.99, 10000],\n",
    "    [0.25, 0.99, 0.01, 0.997, 0.99, 8000],\n",
    "    [1, 0.95, 0.1, 0.96, 0.96, 12000],\n",
    "    [0.9, 0.9, 0.005, 0.95, 0.999, 10000],\n",
    "    [0.05, 0.85, 0.01, 0.997, 0.95, 4000]\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "'''\n",
    "alpha = 0.7\n",
    "gamma = 0.75\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.97\n",
    "decay_rate = 0.97\n",
    "n_episodes = 28000\n",
    "n_states = states.shape[0]  # Number of states\n",
    "n_actions = len(actions)  # Number of actions\n",
    "'''\n",
    "alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes = Hyperparameters[5] # Hyperparameters[21]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_index_mapping(df):\n",
    "    state_to_index = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        state = (row['sma-05'], row['sma-07'], row['sma-25'], row['sma-compare'], row['is_short'])\n",
    "        state_to_index[state] = idx\n",
    "    return state_to_index\n",
    "\n",
    "# Assuming 'df' is your dataframe used during training\n",
    "state_to_index = create_state_index_mapping(train_data)\n",
    "\n",
    "# Save the state_to_index dictionary for later use\n",
    "np.save('small_state_to_index.npy', state_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to choose an action using epsilon-greedy\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, n_actions)  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit: best known action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Set random seed for reproducibility and train the loop\\nnp.random.seed(42)\\nrandom.seed(42)\\n# Initialize a list to store rewards per episode\\nrewards_per_episode = []\\n\\nfor episode in tqdm(range(n_episodes), desc=\"evaluating results per episode ...\"):\\n    current_state = np.random.randint(0, n_states)  # Random initial state\\n    total_reward = 0  # Initialize total reward for the current episode\\n\\n    while current_state < n_states - 1:\\n        action = choose_action(current_state, epsilon)\\n        \\n        next_state = current_state + 1  # This depends on your environment logic\\n        reward = rewards[next_state]\\n\\n        best_next_action = np.argmax(q_table[next_state])\\n        q_table[current_state, action] += alpha * (\\n            reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\\n        )\\n        \\n        total_reward += reward  # Accumulate reward for the current episode\\n        current_state = next_state  # Move to next state\\n\\n    rewards_per_episode.append(total_reward)  # Store the total reward for the current episode\\n\\n    # Decay epsilon\\n    epsilon = max(min_epsilon, epsilon * decay_rate)\\n\\n    # Optional: Log progress\\n    if episode % 1000 == 0:  # Adjust logging frequency as needed\\n        print(f\"Episode {episode}/{n_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\\n\\n# Example: Save the Q-table\\nnp.save(\"small_q_table.npy\", q_table)\\n\\n# Example: Plotting the rewards\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(rewards_per_episode)\\nplt.xlabel(\\'Episode\\')\\nplt.ylabel(\\'Reward\\')\\nplt.title(\\'Rewards per Episode\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Set random seed for reproducibility and train the loop\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "# Initialize a list to store rewards per episode\n",
    "rewards_per_episode = []\n",
    "\n",
    "for episode in tqdm(range(n_episodes), desc=\"evaluating results per episode ...\"):\n",
    "    current_state = np.random.randint(0, n_states)  # Random initial state\n",
    "    total_reward = 0  # Initialize total reward for the current episode\n",
    "\n",
    "    while current_state < n_states - 1:\n",
    "        action = choose_action(current_state, epsilon)\n",
    "        \n",
    "        next_state = current_state + 1  # This depends on your environment logic\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        q_table[current_state, action] += alpha * (\n",
    "            reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "        )\n",
    "        \n",
    "        total_reward += reward  # Accumulate reward for the current episode\n",
    "        current_state = next_state  # Move to next state\n",
    "\n",
    "    rewards_per_episode.append(total_reward)  # Store the total reward for the current episode\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if episode % 1000 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Episode {episode}/{n_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "# Example: Save the Q-table\n",
    "np.save(\"small_q_table.npy\", q_table)\n",
    "\n",
    "# Example: Plotting the rewards\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:   0%|          | 13/8000 [00:00<01:04, 123.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/8000 - Total Reward: -1835.2951134899984, Epsilon: 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  10%|█         | 820/8000 [00:06<01:02, 114.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800/8000 - Total Reward: -3709.611868039993, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  20%|██        | 1622/8000 [00:12<00:50, 126.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600/8000 - Total Reward: 766.4187404299995, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  30%|███       | 2422/8000 [00:19<00:51, 109.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400/8000 - Total Reward: 806.3781415799994, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  40%|████      | 3227/8000 [00:25<00:34, 139.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200/8000 - Total Reward: 668.8249095999996, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  50%|█████     | 4015/8000 [00:31<00:34, 114.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000/8000 - Total Reward: 739.7775658999998, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  60%|██████    | 4820/8000 [00:38<00:24, 128.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4800/8000 - Total Reward: 421.3456069000006, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  70%|███████   | 5614/8000 [00:44<00:19, 120.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5600/8000 - Total Reward: -101.15944664000082, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  80%|████████  | 6415/8000 [00:50<00:11, 132.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6400/8000 - Total Reward: -4725.606905659993, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...:  90%|█████████ | 7220/8000 [00:57<00:05, 132.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7200/8000 - Total Reward: 232.7245574499997, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating results per episode ...: 100%|██████████| 8000/8000 [01:03<00:00, 126.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simplified environment function\n",
    "def environment_step(current_state, action):\n",
    "    \"\"\"\n",
    "    Simulates the environment's response to an action.\n",
    "    \n",
    "    Args:\n",
    "        current_state (int): The current state of the environment.\n",
    "        action (int): The action taken by the agent.\n",
    "    \n",
    "    Returns:\n",
    "        next_state (int): The next state after taking the action.\n",
    "        reward (float): The reward received after taking the action.\n",
    "    \"\"\"\n",
    "    # Define the environment logic here\n",
    "    next_state = current_state + 1  # Example: Move to the next state\n",
    "    reward = rewards[next_state]    # Example: Reward is based on the next state\n",
    "    \n",
    "    return next_state, reward\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize a list to store rewards per episode\n",
    "rewards_per_episode = []\n",
    "\n",
    "# Training loop\n",
    "for episode in tqdm(range(n_episodes), desc=\"evaluating results per episode ...\"):\n",
    "    current_state = np.random.randint(0, n_states)  # Random initial state\n",
    "    total_reward = 0  # Initialize total reward for the current episode\n",
    "\n",
    "    while current_state < n_states - 1:\n",
    "        action = choose_action(current_state, epsilon)\n",
    "        \n",
    "        # Use the environment function to get the next state and reward\n",
    "        next_state, reward = environment_step(current_state, action)\n",
    "\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        q_table[current_state, action] += alpha * (\n",
    "            reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "        )\n",
    "        \n",
    "        total_reward += reward  # Accumulate reward for the current episode\n",
    "        current_state = next_state  # Move to next state\n",
    "\n",
    "    rewards_per_episode.append(total_reward)  # Store the total reward for the current episode\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if episode % 800 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Episode {episode}/{n_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "# Example: Save the Q-table\n",
    "np.save(\"small_q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_q_table(file_path):\n",
    "    return np.load(file_path)\n",
    "\n",
    "def load_state_index_mapping(file_path):\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "loaded_mapping = load_state_index_mapping(file_path=\"small_state_to_index.npy\")\n",
    "loaded_qtable = load_q_table(file_path=\"small_q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_state(sma_05: float, sma_07: float, sma_25: float, sma_compare: int, is_short: int):\n",
    "    state = np.array([[sma_05, sma_07, sma_25, sma_compare, is_short]])\n",
    "    if not np.all(np.isfinite(state)):\n",
    "        state = np.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_action(state, q_table, state_to_index, action_mapping, default_action: str = None):\n",
    "    state_tuple = tuple(state.flatten())\n",
    "\n",
    "    state_index = state_to_index.get(state_tuple, -1)\n",
    "\n",
    "    if not state_index == -1:\n",
    "        try:\n",
    "            q_values = q_table[state_index]\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            #return default_action\n",
    "    else:\n",
    "        state_tuples = list(state_to_index.keys())\n",
    "        kdtree = KDTree(state_tuples)\n",
    "        distance, index = kdtree.query(state.flatten())\n",
    "        nearest_state_tuple = state_tuples[index]\n",
    "        new_state_index = state_to_index[nearest_state_tuple]\n",
    "        q_values = loaded_qtable[new_state_index]\n",
    "    \n",
    "    #q_values = q_table[state_index]\n",
    "    best_action_index = np.argmax(q_values)\n",
    "    action = [action for action, index in action_mapping.items() if index == best_action_index][0]\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "with open(\"small_q_table.npy\", \"rb\") as f:\n",
    "    q_table = load_q_table(\"small_q_table.npy\")\n",
    "\n",
    "with open(\"small_state_to_index.npy\", \"rb\") as f:\n",
    "    state_to_index = load_state_index_mapping(\"small_state_to_index.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values\n",
    "state_1 = list(X[-1:].flatten()) # sample: [[0.87024    0.85277143 0.779504   0.         1.        ]]\n",
    "\n",
    "state = prep_state(*state_1)\n",
    "action_mapping = {\"go_long\": 0, \"go_short\": 1, \"do_nothing\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted action for the state is: go_long\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predicted_action = predict_action(state, q_table, state_to_index, action_mapping)\n",
    "    print(f\"The predicted action for the state is: {predicted_action}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"The state is not found in the state index mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict an entire range\n",
    "for idx, row in train_data.iterrows():\n",
    "    state = row[[\"sma-05\", \"sma-07\", \"sma-25\", \"sma-compare\", \"is_short\"]].values\n",
    "    action = predict_action(state, q_table, state_to_index, action_mapping)\n",
    "    train_data.loc[idx, \"predicted_action\"] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed:</th>\n",
       "      <th>sma-05</th>\n",
       "      <th>sma-07</th>\n",
       "      <th>sma-25</th>\n",
       "      <th>sma-compare</th>\n",
       "      <th>is_short</th>\n",
       "      <th>action</th>\n",
       "      <th>imit-action</th>\n",
       "      <th>nlpreds</th>\n",
       "      <th>reward</th>\n",
       "      <th>action_num</th>\n",
       "      <th>predicted_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.502460</td>\n",
       "      <td>0.497686</td>\n",
       "      <td>0.488548</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>5.984069</td>\n",
       "      <td>0</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181480</td>\n",
       "      <td>0.180829</td>\n",
       "      <td>0.178788</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.372316</td>\n",
       "      <td>0</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.025259</td>\n",
       "      <td>0.025356</td>\n",
       "      <td>0.026236</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>4.147186</td>\n",
       "      <td>0</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.612120</td>\n",
       "      <td>1.603329</td>\n",
       "      <td>1.571272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>4.228221</td>\n",
       "      <td>0</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14.172400</td>\n",
       "      <td>14.174857</td>\n",
       "      <td>13.866320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>1.886275</td>\n",
       "      <td>0</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.439160</td>\n",
       "      <td>3.407986</td>\n",
       "      <td>3.334664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>4.486469</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.580220</td>\n",
       "      <td>0.577871</td>\n",
       "      <td>0.564268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>4.802110</td>\n",
       "      <td>0</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.977384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>1.097025</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>634.142000</td>\n",
       "      <td>638.405714</td>\n",
       "      <td>647.532400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>2.375761</td>\n",
       "      <td>0</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.527440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-0.420655</td>\n",
       "      <td>0</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3063.638000</td>\n",
       "      <td>3056.657143</td>\n",
       "      <td>3039.704000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>9.577524</td>\n",
       "      <td>0</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_long</td>\n",
       "      <td>8.643298</td>\n",
       "      <td>0</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>11.991840</td>\n",
       "      <td>12.037743</td>\n",
       "      <td>12.538216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>7.609980</td>\n",
       "      <td>0</td>\n",
       "      <td>go_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.512680</td>\n",
       "      <td>0.507661</td>\n",
       "      <td>0.464642</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>1.019501</td>\n",
       "      <td>0</td>\n",
       "      <td>do_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>18.431800</td>\n",
       "      <td>18.342571</td>\n",
       "      <td>17.877600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_short</td>\n",
       "      <td>go_long</td>\n",
       "      <td>-5.487195</td>\n",
       "      <td>0</td>\n",
       "      <td>go_long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed:       sma-05       sma-07       sma-25  sma-compare  is_short  \\\n",
       "0          0     0.502460     0.497686     0.488548            0         0   \n",
       "1          1     0.181480     0.180829     0.178788            0         1   \n",
       "2          2     0.025259     0.025356     0.026236            1         0   \n",
       "3          3     1.612120     1.603329     1.571272            0         0   \n",
       "4          4    14.172400    14.174857    13.866320            0         0   \n",
       "5          5     3.439160     3.407986     3.334664            0         0   \n",
       "6          6     0.580220     0.577871     0.564268            0         0   \n",
       "7          7     0.999300     0.993443     0.977384            0         0   \n",
       "8          8   634.142000   638.405714   647.532400            1         0   \n",
       "9          9     0.540400     0.539000     0.527440            0         0   \n",
       "10        10  3063.638000  3056.657143  3039.704000            0         0   \n",
       "11        11     0.002054     0.002052     0.002040            0         0   \n",
       "12        12    11.991840    12.037743    12.538216            1         0   \n",
       "13        13     0.512680     0.507661     0.464642            0         0   \n",
       "14        14    18.431800    18.342571    17.877600            0         1   \n",
       "\n",
       "      action imit-action  nlpreds    reward  action_num predicted_action  \n",
       "0    go_long    go_short  go_long  5.984069           0       do_nothing  \n",
       "1   go_short    go_short  go_long -5.372316           0       do_nothing  \n",
       "2    go_long     go_long  go_long  4.147186           0       do_nothing  \n",
       "3    go_long    go_short  go_long  4.228221           0         go_short  \n",
       "4    go_long    go_short  go_long  1.886275           0         go_short  \n",
       "5    go_long    go_short  go_long  4.486469           0          go_long  \n",
       "6    go_long    go_short  go_long  4.802110           0         go_short  \n",
       "7    go_long    go_short  go_long  1.097025           0          go_long  \n",
       "8    go_long    go_short  go_long  2.375761           0       do_nothing  \n",
       "9    go_long    go_short  go_long -0.420655           0         go_short  \n",
       "10   go_long    go_short  go_long  9.577524           0       do_nothing  \n",
       "11   go_long     go_long  go_long  8.643298           0         go_short  \n",
       "12   go_long    go_short  go_long  7.609980           0         go_short  \n",
       "13   go_long    go_short  go_long  1.019501           0       do_nothing  \n",
       "14  go_short    go_short  go_long -5.487195           0          go_long  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_action\n",
       "go_long       1540\n",
       "go_short       489\n",
       "do_nothing     476\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['predicted_action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_data[(train_data['nlpreds'] == 'go_short') & (train_data['reward'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_short\n",
       "1    166\n",
       "0      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['is_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlpreds\n",
       "go_long     1070\n",
       "go_short     168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df0[(df0['reward'] > 0)]\n",
    "s['nlpreds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed state 0/2504\n",
      "Current Predicted Reward: 0\n",
      "Current Actual Reward: -5.37231572\n",
      "Processed state 100/2504\n",
      "Current Predicted Reward: 303.23515544\n",
      "Current Actual Reward: 1080.5410523199998\n",
      "Processed state 200/2504\n",
      "Current Predicted Reward: 692.34382102\n",
      "Current Actual Reward: 2310.652637359999\n",
      "Processed state 300/2504\n",
      "Current Predicted Reward: 1862.6532551700006\n",
      "Current Actual Reward: 3550.5854586099986\n",
      "Processed state 400/2504\n",
      "Current Predicted Reward: 2770.7781411100013\n",
      "Current Actual Reward: 4798.93971933\n",
      "Processed state 500/2504\n",
      "Current Predicted Reward: 3107.996669340001\n",
      "Current Actual Reward: 6041.297320040003\n",
      "Processed state 600/2504\n",
      "Current Predicted Reward: 3456.5634272800003\n",
      "Current Actual Reward: 7138.83253316\n",
      "Processed state 700/2504\n",
      "Current Predicted Reward: 3186.7944195600007\n",
      "Current Actual Reward: 6326.089369459994\n",
      "Processed state 800/2504\n",
      "Current Predicted Reward: 2594.980050100001\n",
      "Current Actual Reward: 4634.717703039992\n",
      "Processed state 900/2504\n",
      "Current Predicted Reward: 2098.6006075500013\n",
      "Current Actual Reward: 2981.1153774699924\n",
      "Processed state 1000/2504\n",
      "Current Predicted Reward: 1650.0999340100013\n",
      "Current Actual Reward: 1951.5440587099924\n",
      "Processed state 1100/2504\n",
      "Current Predicted Reward: 1542.8259457800013\n",
      "Current Actual Reward: 1601.0054936599922\n",
      "Processed state 1200/2504\n",
      "Current Predicted Reward: 1485.1717164400013\n",
      "Current Actual Reward: 1678.143773369992\n",
      "Processed state 1300/2504\n",
      "Current Predicted Reward: 1416.6889108000014\n",
      "Current Actual Reward: 1410.5156781799915\n",
      "Processed state 1400/2504\n",
      "Current Predicted Reward: 1163.5401738600012\n",
      "Current Actual Reward: 1120.1742729799914\n",
      "Processed state 1500/2504\n",
      "Current Predicted Reward: 1072.4860058800014\n",
      "Current Actual Reward: 1091.121574829992\n",
      "Processed state 1600/2504\n",
      "Current Predicted Reward: 1021.9630156900012\n",
      "Current Actual Reward: 963.2704335699923\n",
      "Processed state 1700/2504\n",
      "Current Predicted Reward: 999.9661948600011\n",
      "Current Actual Reward: 1000.9759787199927\n",
      "Processed state 1800/2504\n",
      "Current Predicted Reward: 1033.199020320001\n",
      "Current Actual Reward: 1034.2088041799925\n",
      "Processed state 1900/2504\n",
      "Current Predicted Reward: 1200.2471979100005\n",
      "Current Actual Reward: 1201.256981769992\n",
      "Processed state 2000/2504\n",
      "Current Predicted Reward: 1065.579647550001\n",
      "Current Actual Reward: 1066.5894314099924\n",
      "Processed state 2100/2504\n",
      "Current Predicted Reward: 947.1917804000007\n",
      "Current Actual Reward: 948.201564259992\n",
      "Processed state 2200/2504\n",
      "Current Predicted Reward: 984.5960917399998\n",
      "Current Actual Reward: 985.6058755999911\n",
      "Processed state 2300/2504\n",
      "Current Predicted Reward: 1400.0895735699994\n",
      "Current Actual Reward: 1401.0993574299907\n",
      "Processed state 2400/2504\n",
      "Current Predicted Reward: 1818.133295919998\n",
      "Current Actual Reward: 1819.1430797799894\n",
      "Processed state 2500/2504\n",
      "Current Predicted Reward: 1833.5861855999976\n",
      "Current Actual Reward: 1834.595969459989\n",
      "Cumulative Predicted Reward: 1836.8050151299976\n",
      "Cumulative Actual Reward: 1837.814798989989\n",
      "Prediction Efficiency: -0.05%\n"
     ]
    }
   ],
   "source": [
    "# Performance measures\n",
    "# Initialize cumulative rewards\n",
    "cumulative_predicted_reward = 0\n",
    "cumulative_actual_reward = 0\n",
    "\n",
    "# Iterate through states to calculate rewards\n",
    "for state_index in range(n_states - 1):\n",
    "    # Predicted action from Q-table\n",
    "    predicted_action = np.argmax(q_table[state_index])  # Best action for the current state\n",
    "    # Actual action from the ground truth\n",
    "    actual_action = train_data[\"action_num\"].iloc[state_index]\n",
    "\n",
    "    # Get reward for predicted action only if it matches the actual action\n",
    "    if predicted_action == actual_action:\n",
    "        predicted_reward = rewards[state_index + 1]  # Reward for the correct prediction\n",
    "        cumulative_predicted_reward += predicted_reward\n",
    "\n",
    "    # Get actual reward for the ground truth action\n",
    "    actual_reward = rewards[state_index + 1]\n",
    "    cumulative_actual_reward += actual_reward\n",
    "\n",
    "    # Optional: Log progress\n",
    "    if state_index % 100 == 0:  # Adjust logging frequency as needed\n",
    "        print(f\"Processed state {state_index}/{n_states - 1}\")\n",
    "        print(f\"Current Predicted Reward: {cumulative_predicted_reward}\")\n",
    "        print(f\"Current Actual Reward: {cumulative_actual_reward}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cumulative Predicted Reward: {cumulative_predicted_reward}\")\n",
    "print(f\"Cumulative Actual Reward: {cumulative_actual_reward}\")\n",
    "\n",
    "# Optionally calculate efficiency\n",
    "efficiency = (\n",
    "    ((cumulative_predicted_reward - cumulative_actual_reward) / abs(cumulative_actual_reward)) * 100\n",
    "    if cumulative_actual_reward != 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "print(f\"Prediction Efficiency: {efficiency:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.84%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "correct_predictions = 0\n",
    "for state_index in range(n_states):\n",
    "    predicted_action = np.argmax(q_table[state_index])  # Predicted action\n",
    "    actual_action = train_data[\"action_num\"].iloc[state_index]  # Actual action\n",
    "    if predicted_action == actual_action:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / n_states\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1433  373  364]\n",
      " [ 107  116  112]\n",
      " [   0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "y_true = train_data[\"action_num\"]  # Actual actions\n",
    "y_pred = [np.argmax(q_table[state_index]) for state_index in range(n_states)]  # Predicted actions\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_reward(action: str, is_short: int):\n",
    "    m = train_data[(train_data['predicted_action'] == action) & (train_data['is_short'] == is_short)]\n",
    "    counts = m['is_short'].value_counts()\n",
    "    total_reward = m['reward'].cumsum()[-1:].values[0]\n",
    "    wins = len(m[m['reward'] > 0])\n",
    "    losses = len(m[m['reward'] <= 0])\n",
    "    return {\n",
    "        'counts': counts.get(is_short),\n",
    "        'total_reward': total_reward,\n",
    "        'winrate': f'{wins * 100 / (losses + wins):.2f}%'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'counts': 126, 'total_reward': -148.67104949000012, 'winrate': '49.21%'}\n"
     ]
    }
   ],
   "source": [
    "print(action_reward('go_short', 1)) # go_short 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334    3.629589\n",
       "Name: reward, dtype: float64"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = m[m['is_short'] == 0]\n",
    "m['reward'].cumsum()[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of rewards for each episode\n",
    "#rewards_per_episode = [...]  # Populate this with your actual data\n",
    "\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "def train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards):\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        current_state = np.random.randint(0, n_states)\n",
    "        total_reward = 0\n",
    "\n",
    "        while current_state < n_states - 1:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "\n",
    "            next_state = current_state + 1  # Adjust based on environment logic\n",
    "            reward = rewards[next_state]\n",
    "\n",
    "            best_next_action = np.argmax(q_table[next_state])\n",
    "            q_table[current_state, action] += alpha * (\n",
    "                reward + gamma * q_table[next_state, best_next_action] - q_table[current_state, action]\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    return q_table, rewards_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards):\n",
    "    correct_predictions = 0\n",
    "    total_reward = 0\n",
    "    reward_weighted_accuracy = []\n",
    "\n",
    "    for state_index in range(n_states):\n",
    "        predicted_action = np.argmax(q_table[state_index])  # Predicted action\n",
    "        actual_action = train_data[\"action_num\"].iloc[state_index]  # Actual action\n",
    "        reward = rewards[state_index]  # Reward for the action\n",
    "\n",
    "        if predicted_action == actual_action:\n",
    "            correct_predictions += 1\n",
    "            total_reward += reward\n",
    "\n",
    "        accuracy = correct_predictions / (state_index + 1)\n",
    "        reward_weighted_accuracy.append(total_reward / (state_index + 1))\n",
    "\n",
    "        # Optional: Log progress\n",
    "        if state_index % 100 == 0:\n",
    "            print(f\"Processed state {state_index}/{n_states} - Accuracy: {accuracy * 100:.2f}%, Reward-weighted Accuracy: {reward_weighted_accuracy[-1]}\")\n",
    "\n",
    "    final_reward_weighted_accuracy = total_reward / n_states\n",
    "    return final_reward_weighted_accuracy * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards):\n",
    "    # Initialize cumulative rewards\n",
    "    cumulative_predicted_reward = 0\n",
    "    cumulative_actual_reward = 0\n",
    "\n",
    "    # Iterate through states to calculate rewards\n",
    "    for state_index in range(n_states - 1):\n",
    "        # Predicted action from Q-table\n",
    "        predicted_action = np.argmax(q_table[state_index])  # Best action for the current state\n",
    "        # Actual action from the ground truth\n",
    "        actual_action = train_data[\"action_num\"].iloc[state_index]\n",
    "\n",
    "        # Get reward for predicted action only if it matches the actual action\n",
    "        if predicted_action == actual_action:\n",
    "            predicted_reward = rewards[state_index + 1]  # Reward for the correct prediction\n",
    "            cumulative_predicted_reward += predicted_reward\n",
    "\n",
    "        # Get actual reward for the ground truth action\n",
    "        actual_reward = rewards[state_index + 1]\n",
    "        cumulative_actual_reward += actual_reward\n",
    "    return cumulative_predicted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef random_search_reward_weighted(n_iter, param_grid, n_states, n_actions, rewards, train_data):\\n    best_params = None\\n    best_reward_weighted_accuracy = float(\\'-inf\\')\\n\\n    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\\n        alpha = random.choice(param_grid[\\'alpha\\'])\\n        gamma = random.choice(param_grid[\\'gamma\\'])\\n        epsilon = random.choice(param_grid[\\'epsilon\\'])\\n        min_epsilon = random.choice(param_grid[\\'decay_rate\\'])\\n        decay_rate = random.choice(param_grid[\\'decay_rate\\'])\\n        n_episodes = random.choice(param_grid[\\'n_episodes\\'])\\n\\n        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\\n        reward_weighted_accuracy = evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards)\\n\\n        if reward_weighted_accuracy > best_reward_weighted_accuracy:\\n            best_reward_weighted_accuracy = reward_weighted_accuracy\\n            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\\n\\n        print(f\"Iteration Reward-weighted Accuracy: {reward_weighted_accuracy:.2f}%, Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\\n\\n    return best_params, best_reward_weighted_accuracy\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'alpha\\': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\\n    \\'gamma\\': [0.75, 0.85, 0.9, 0.95, 0.99],\\n    \\'epsilon\\': [1.0, 0.5, 0.1, 0.05, 0.01],\\n    \\'min_epsilon\\': [0.05, 0.01, 0.005, 0.001, 0.001],\\n    \\'decay_rate\\': [0.95, 0.96, 0.97, 0.99, 0.995, 0.997, 0.999],\\n    \\'n_episodes\\': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 18000, 21000, 24000, 28000]\\n}\\n\\n# Perform Random Search\\nbest_params, best_reward_weighted_accuracy = random_search_reward_weighted(50, param_grid, n_states, n_actions, rewards, train_data)\\nprint(f\"Best Hyperparameters: {best_params}\")\\nprint(f\"Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\\n'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def random_search_reward_weighted(n_iter, param_grid, n_states, n_actions, rewards, train_data):\n",
    "    best_params = None\n",
    "    best_reward_weighted_accuracy = float('-inf')\n",
    "\n",
    "    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\n",
    "        alpha = random.choice(param_grid['alpha'])\n",
    "        gamma = random.choice(param_grid['gamma'])\n",
    "        epsilon = random.choice(param_grid['epsilon'])\n",
    "        min_epsilon = random.choice(param_grid['decay_rate'])\n",
    "        decay_rate = random.choice(param_grid['decay_rate'])\n",
    "        n_episodes = random.choice(param_grid['n_episodes'])\n",
    "\n",
    "        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\n",
    "        reward_weighted_accuracy = evaluate_q_learning_reward_weighted(q_table, n_states, train_data, rewards)\n",
    "\n",
    "        if reward_weighted_accuracy > best_reward_weighted_accuracy:\n",
    "            best_reward_weighted_accuracy = reward_weighted_accuracy\n",
    "            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\n",
    "\n",
    "        print(f\"Iteration Reward-weighted Accuracy: {reward_weighted_accuracy:.2f}%, Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\n",
    "\n",
    "    return best_params, best_reward_weighted_accuracy\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'gamma': [0.75, 0.85, 0.9, 0.95, 0.99],\n",
    "    'epsilon': [1.0, 0.5, 0.1, 0.05, 0.01],\n",
    "    'min_epsilon': [0.05, 0.01, 0.005, 0.001, 0.001],\n",
    "    'decay_rate': [0.95, 0.96, 0.97, 0.99, 0.995, 0.997, 0.999],\n",
    "    'n_episodes': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 18000, 21000, 24000, 28000]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "best_params, best_reward_weighted_accuracy = random_search_reward_weighted(50, param_grid, n_states, n_actions, rewards, train_data)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Reward-weighted Accuracy: {best_reward_weighted_accuracy:.2f}%\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef random_search_prediction_efficiency(n_iter, param_grid, n_states, n_actions, rewards, train_data):\\n    best_params = None\\n    best_cumulative_pred_reward = float(\\'-inf\\')\\n\\n    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\\n        alpha = random.choice(param_grid[\\'alpha\\'])\\n        gamma = random.choice(param_grid[\\'gamma\\'])\\n        epsilon = random.choice(param_grid[\\'epsilon\\'])\\n        min_epsilon = random.choice(param_grid[\\'decay_rate\\'])\\n        decay_rate = random.choice(param_grid[\\'decay_rate\\'])\\n        n_episodes = random.choice(param_grid[\\'n_episodes\\'])\\n\\n        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\\n        cumulative_pred_reward = evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards)\\n\\n        if cumulative_pred_reward > best_cumulative_pred_reward:\\n            best_cumulative_pred_reward = cumulative_pred_reward\\n            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\\n\\n        print(f\"Iteration cumulative predicted reward: {cumulative_pred_reward:.2f}%, Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\\n\\n    return best_params, best_cumulative_pred_reward\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\'alpha\\': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\\n    \\'gamma\\': [0.75, 0.85, 0.9, 0.95, 0.99],\\n    \\'epsilon\\': [1.0, 0.5, 0.1, 0.05, 0.01, 0.005],\\n    \\'min_epsilon\\': [0.05, 0.01, 0.005, 0.001, 0.001],\\n    \\'decay_rate\\': [0.95, 0.99, 0.995, 0.997, 0.999],\\n    \\'n_episodes\\': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 16000, 18000, 20000, 22000, 24000, 26000, 28000, 30000]\\n}\\n\\n# Perform Random Search\\nbest_params, best_cumulative_pred_reward = random_search_prediction_efficiency(50, param_grid, n_states, n_actions, rewards, train_data)\\nprint(f\"Best Hyperparameters: {best_params}\")\\nprint(f\"Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\\n'"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def random_search_prediction_efficiency(n_iter, param_grid, n_states, n_actions, rewards, train_data):\n",
    "    best_params = None\n",
    "    best_cumulative_pred_reward = float('-inf')\n",
    "\n",
    "    for _ in tqdm(range(n_iter), desc=\"Searching for params ...\"):\n",
    "        alpha = random.choice(param_grid['alpha'])\n",
    "        gamma = random.choice(param_grid['gamma'])\n",
    "        epsilon = random.choice(param_grid['epsilon'])\n",
    "        min_epsilon = random.choice(param_grid['decay_rate'])\n",
    "        decay_rate = random.choice(param_grid['decay_rate'])\n",
    "        n_episodes = random.choice(param_grid['n_episodes'])\n",
    "\n",
    "        q_table, _ = train_q_learning(alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes, n_states, n_actions, rewards)\n",
    "        cumulative_pred_reward = evaluate_q_learning_prediction_efficiency(q_table, n_states, train_data, rewards)\n",
    "\n",
    "        if cumulative_pred_reward > best_cumulative_pred_reward:\n",
    "            best_cumulative_pred_reward = cumulative_pred_reward\n",
    "            best_params = (alpha, gamma, epsilon, min_epsilon, decay_rate, n_episodes)\n",
    "\n",
    "        print(f\"Iteration cumulative predicted reward: {cumulative_pred_reward:.2f}%, Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\n",
    "\n",
    "    return best_params, best_cumulative_pred_reward\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'gamma': [0.75, 0.85, 0.9, 0.95, 0.99],\n",
    "    'epsilon': [1.0, 0.5, 0.1, 0.05, 0.01, 0.005],\n",
    "    'min_epsilon': [0.05, 0.01, 0.005, 0.001, 0.001],\n",
    "    'decay_rate': [0.95, 0.99, 0.995, 0.997, 0.999],\n",
    "    'n_episodes': [1500, 4000,6000, 8000, 9000, 10000, 11000, 12000, 14000, 16000, 18000, 20000, 22000, 24000, 26000, 28000, 30000]\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "best_params, best_cumulative_pred_reward = random_search_prediction_efficiency(50, param_grid, n_states, n_actions, rewards, train_data)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best cumulative predicted reward: {best_cumulative_pred_reward:.2f}%\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
